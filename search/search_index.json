{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"AI regulation and governance This section contains resources about the regulation and governance of artificial intelligence (AI). It covers policies in various jusrisdictions, as well as standards, guidelines, recommended practices and frameworks for the promotion and assurance of trustworthy AI. The contents are organised in the following sections: EU: This section summarises the proposed EU AI act. China: This section describes the principles for AI governance and standardisation framework in China. UK: This section descibes the UK roadmap to an effective AI assurance ecosystem. New Zealand: This section describes thye principles approach of the New Zealand government to AI governance. ISO/IEC: This section describes important AI related ISO/IEC standards that are published or under development. IEEE: TBD OECD: TBD DNV: TBD Definitions: This section dicuss important definitions related to AI and assurance of AI.","title":"Introduction"},{"location":"#ai-regulation-and-governance","text":"This section contains resources about the regulation and governance of artificial intelligence (AI). It covers policies in various jusrisdictions, as well as standards, guidelines, recommended practices and frameworks for the promotion and assurance of trustworthy AI. The contents are organised in the following sections: EU: This section summarises the proposed EU AI act. China: This section describes the principles for AI governance and standardisation framework in China. UK: This section descibes the UK roadmap to an effective AI assurance ecosystem. New Zealand: This section describes thye principles approach of the New Zealand government to AI governance. ISO/IEC: This section describes important AI related ISO/IEC standards that are published or under development. IEEE: TBD OECD: TBD DNV: TBD Definitions: This section dicuss important definitions related to AI and assurance of AI.","title":"AI regulation and governance"},{"location":"china/","text":"AI governance and standardization in China In 2019, China's New Generation AI Governance Expert Committee published \"Eight principles for AI governance and \u201cresponsible AI\u201d . The motivation behind the proposed principles is the healthy development of a new generation of AI. If followed, one can ensure that AI is safe/secure, and reliable. The eight principles are as follows: Harmony and friendliness: The primary objective of AI should be to enhance the common well-being of humanity. In other words, AI should serve the progress of human civilization while complying with the ethical and moral dimensions, and upholding the societal security and human rights. Fairness and justice: Biasness and discrimination should be avoided in each phase from data gathering to the final product development of AI. Instead, fairness, justice, and equality of opportunity should be promoted. The rights and interests of stakeholders should be protected. Inclusivity and sharing: AI should be for everyone. Coordinated, shared, and inclusive development of AI should be encouraged to secure the adaptability of people from different backgrounds and abilities. Resources should be openly accessible to evade data and platform monopolies. Respect privacy: In the development of AI, personal privacy should be strictly protected. If personal data is used, necessary boundaries and standards should be enforced to protect them. Contributing individuals should have the right to know and decide which data should be used and where they should be used. Secure/safe and controllable: In order to attain trustworthiness, AI systems should emphasize transparency, explainability, reliability, and controllability. The robustness, safety, and security of AI systems must be given special attention, and an external assessment of these components is needed. Shared responsibility: In general, it is the responsibility of AI developers, users, beneficiaries, and all the stakeholders involved to prevent the misuse of AI against laws, regulations, ethics, morals, standards, and norms. When needed, the specific responsibilities of these parties should also be defined. Open collaboration: To promote the healthy development of AI, knowledge and ideas should be shared and exchanged across disciplines, domains, regions, and borders. For collaborations at the international level, consensus on an international AI governance framework, standards, and norms should be initiated. Agile governance: In order to promote the innovative, orderly, and natural development of AI, management mechanisms, governance systems, and recommender practices should be continuously updated. Future risks associated with the advancements in AI should be anticipated and necessary adjustments should be made accordingly. China has a multi-layered framework for standardisation of AI . Level A - Foundations: Foundational standards include the four main categories of terminology, reference architecture, data, and testing and assessment. These standards supports the other parts in the standards system structure. Level B - Platforms/support: Supportive technology and product standards provide basic support for artificial intelligence software and hardware platform construction, algorithm model development, and artificial intelligence applications. Fundamental hardware and software platform standards mainly focus on intelligent chips, system software, development frameworks, etc., to provide infrastructure support for artificial intelligence. Level C - Key technologies: Key technology standards a cover topics like natural language processing, human-computer interaction, computer vision, biometric feature recognition, and VR/AR. These standards provide support for practical applications of AI. Level D - Products and services: Products/services standards cover 'intelligentized' products and new service models formed in AI technology fields. Level E - Application standard: These standards focus on specific application areas, e.g. smart homes, smart manufacturing, smart healthcare, etc. They focus on needs facing the specific industries, refining other parts of the standards to support the development of various industries. Level F - Security/ethics: These standards run through the other parts to establish a compliance system for AI. In 2021, China published a white paper on Trusted artificial intelligence . It highlights the need for regulations and law to guide AI development, and also describes the use of 3rd parties for evaluations and verifications. It is suggested that AI enterprises and insurance institutions can explore the insurance mechanism of AI product application, conduct quantitative assessment of risk accidents, provide risk compensation, and help improve the trusted AI ecosystem.","title":"China"},{"location":"china/#ai-governance-and-standardization-in-china","text":"In 2019, China's New Generation AI Governance Expert Committee published \"Eight principles for AI governance and \u201cresponsible AI\u201d . The motivation behind the proposed principles is the healthy development of a new generation of AI. If followed, one can ensure that AI is safe/secure, and reliable. The eight principles are as follows: Harmony and friendliness: The primary objective of AI should be to enhance the common well-being of humanity. In other words, AI should serve the progress of human civilization while complying with the ethical and moral dimensions, and upholding the societal security and human rights. Fairness and justice: Biasness and discrimination should be avoided in each phase from data gathering to the final product development of AI. Instead, fairness, justice, and equality of opportunity should be promoted. The rights and interests of stakeholders should be protected. Inclusivity and sharing: AI should be for everyone. Coordinated, shared, and inclusive development of AI should be encouraged to secure the adaptability of people from different backgrounds and abilities. Resources should be openly accessible to evade data and platform monopolies. Respect privacy: In the development of AI, personal privacy should be strictly protected. If personal data is used, necessary boundaries and standards should be enforced to protect them. Contributing individuals should have the right to know and decide which data should be used and where they should be used. Secure/safe and controllable: In order to attain trustworthiness, AI systems should emphasize transparency, explainability, reliability, and controllability. The robustness, safety, and security of AI systems must be given special attention, and an external assessment of these components is needed. Shared responsibility: In general, it is the responsibility of AI developers, users, beneficiaries, and all the stakeholders involved to prevent the misuse of AI against laws, regulations, ethics, morals, standards, and norms. When needed, the specific responsibilities of these parties should also be defined. Open collaboration: To promote the healthy development of AI, knowledge and ideas should be shared and exchanged across disciplines, domains, regions, and borders. For collaborations at the international level, consensus on an international AI governance framework, standards, and norms should be initiated. Agile governance: In order to promote the innovative, orderly, and natural development of AI, management mechanisms, governance systems, and recommender practices should be continuously updated. Future risks associated with the advancements in AI should be anticipated and necessary adjustments should be made accordingly. China has a multi-layered framework for standardisation of AI . Level A - Foundations: Foundational standards include the four main categories of terminology, reference architecture, data, and testing and assessment. These standards supports the other parts in the standards system structure. Level B - Platforms/support: Supportive technology and product standards provide basic support for artificial intelligence software and hardware platform construction, algorithm model development, and artificial intelligence applications. Fundamental hardware and software platform standards mainly focus on intelligent chips, system software, development frameworks, etc., to provide infrastructure support for artificial intelligence. Level C - Key technologies: Key technology standards a cover topics like natural language processing, human-computer interaction, computer vision, biometric feature recognition, and VR/AR. These standards provide support for practical applications of AI. Level D - Products and services: Products/services standards cover 'intelligentized' products and new service models formed in AI technology fields. Level E - Application standard: These standards focus on specific application areas, e.g. smart homes, smart manufacturing, smart healthcare, etc. They focus on needs facing the specific industries, refining other parts of the standards to support the development of various industries. Level F - Security/ethics: These standards run through the other parts to establish a compliance system for AI. In 2021, China published a white paper on Trusted artificial intelligence . It highlights the need for regulations and law to guide AI development, and also describes the use of 3rd parties for evaluations and verifications. It is suggested that AI enterprises and insurance institutions can explore the insurance mechanism of AI product application, conduct quantitative assessment of risk accidents, provide risk compensation, and help improve the trusted AI ecosystem.","title":"AI governance and standardization in China"},{"location":"definitions/","text":"TBD","title":"Definitions"},{"location":"dnv/","text":"TBD","title":"DNV"},{"location":"eu/","text":"AI governance and regulation in the European union The European strategy on AI TBD (Andreas) High-Level Expert Group on AI \u2013 Ethics Guidelines for trustworthy AI The European Commission\u2019s High-Level Expert Group on AI published Ethics Guidelines for trustworthy AI in 2019. It introduces three necessary (but not sufficient) components for achievement of trustworthy AI, that should be met throughout the AI system's entire life cycle, namely that it should be lawful , ethical and robust . The guideline present a framework addressing the ethics and robustness aspects. Chapter I (Foundations of Trustworthy AI) identifies the ethical principles and their correlated values that must be respected in the development, deployment and use of AI systems. Key ethical principles include respect for human autonomy , prevention of harm , fairness and explicability . Chapter II (Realising Trustworthy AI) provides guidance on how Trustworthy AI can be realised, by listing seven requirements that AI systems should meet: human agency and oversight; technical robustness and safety; privacy and data governance; transparency; diversity, non-discrimination and fairness; environmental and societal well-being, and; accountability. Chapter III (Assessing Trustworthy AI) provides a concrete and non-exhaustive Trustworthy AI assessment list aimed at operationalising the key requirements set out in Chapter II. The EU AI Act The AI act proposed by the European Commission in April 2021 is the world\u2019s first legal framework for AI, with potential approval in 2022 and potential entry-into-force in 2023. After passing the act there will be a two-year implementation period. Systems existing at the time of implementation are exempted from the requirements in the act unless they subsequently experience a significant change in purpose or design. The AI act will apply \u201cwhere the output produced by the system is used in the Union\u201d - irrespective of where the provider and user are located. The proposed AI act is risk based and classifies AI systems into four risk categories, and a screening is required to determine the level of regulation: Unacceptable risk: These are prohibited. And include social scoring systems, subliminal techniques for manipulation, exploitation of children or mentally disabled, etc. High risk: These are permitted subject to compliance with AI requirements and conformity assessments. Limited risk: These are permitted, but subject to information/transparency requirements. Minimal/no risk: These are permitted with no restrictions, and application of AI requirements are voluntary. Two categories of high-risk AI are defined: 1. Safety components of regulated products , which are subject to third-party assessment under the relevant sectorial legislation. 2. Stand-alone AI systems in 8 different fields, namely: * Biometric identification of natural persons * Management and operation of critical infrastructure * Educational and vocational training (to determine access, assessment, admission) * Employment, workers management and access to self-employment * Access to and enjoyment of essential private services and public services and benefits * Law enforcement * Migration, asylum and border control management * Administration of justice and democratic processes For high risk AI there are requirements to the management system (article 9), data and data governance (article 10), technical documentation (article 11), record keeping (article 12) transparency 8article 13), human oversight (article 14) and accuracy, robustness and cybersecurity 8article 15). TBD: role of notified bodies.","title":"EU"},{"location":"eu/#ai-governance-and-regulation-in-the-european-union","text":"","title":"AI governance and regulation in the European union"},{"location":"eu/#the-european-strategy-on-ai","text":"TBD (Andreas)","title":"The European strategy on AI"},{"location":"eu/#high-level-expert-group-on-ai-ethics-guidelines-for-trustworthy-ai","text":"The European Commission\u2019s High-Level Expert Group on AI published Ethics Guidelines for trustworthy AI in 2019. It introduces three necessary (but not sufficient) components for achievement of trustworthy AI, that should be met throughout the AI system's entire life cycle, namely that it should be lawful , ethical and robust . The guideline present a framework addressing the ethics and robustness aspects. Chapter I (Foundations of Trustworthy AI) identifies the ethical principles and their correlated values that must be respected in the development, deployment and use of AI systems. Key ethical principles include respect for human autonomy , prevention of harm , fairness and explicability . Chapter II (Realising Trustworthy AI) provides guidance on how Trustworthy AI can be realised, by listing seven requirements that AI systems should meet: human agency and oversight; technical robustness and safety; privacy and data governance; transparency; diversity, non-discrimination and fairness; environmental and societal well-being, and; accountability. Chapter III (Assessing Trustworthy AI) provides a concrete and non-exhaustive Trustworthy AI assessment list aimed at operationalising the key requirements set out in Chapter II.","title":"High-Level Expert Group on AI \u2013 Ethics Guidelines for trustworthy AI"},{"location":"eu/#the-eu-ai-act","text":"The AI act proposed by the European Commission in April 2021 is the world\u2019s first legal framework for AI, with potential approval in 2022 and potential entry-into-force in 2023. After passing the act there will be a two-year implementation period. Systems existing at the time of implementation are exempted from the requirements in the act unless they subsequently experience a significant change in purpose or design. The AI act will apply \u201cwhere the output produced by the system is used in the Union\u201d - irrespective of where the provider and user are located. The proposed AI act is risk based and classifies AI systems into four risk categories, and a screening is required to determine the level of regulation: Unacceptable risk: These are prohibited. And include social scoring systems, subliminal techniques for manipulation, exploitation of children or mentally disabled, etc. High risk: These are permitted subject to compliance with AI requirements and conformity assessments. Limited risk: These are permitted, but subject to information/transparency requirements. Minimal/no risk: These are permitted with no restrictions, and application of AI requirements are voluntary. Two categories of high-risk AI are defined: 1. Safety components of regulated products , which are subject to third-party assessment under the relevant sectorial legislation. 2. Stand-alone AI systems in 8 different fields, namely: * Biometric identification of natural persons * Management and operation of critical infrastructure * Educational and vocational training (to determine access, assessment, admission) * Employment, workers management and access to self-employment * Access to and enjoyment of essential private services and public services and benefits * Law enforcement * Migration, asylum and border control management * Administration of justice and democratic processes For high risk AI there are requirements to the management system (article 9), data and data governance (article 10), technical documentation (article 11), record keeping (article 12) transparency 8article 13), human oversight (article 14) and accuracy, robustness and cybersecurity 8article 15). TBD: role of notified bodies.","title":"The EU AI Act"},{"location":"ieee/","text":"TBD","title":"IEEE"},{"location":"iso/","text":"ISO/IEC standardization of AI The ISO/IEC JTC 1/SC 42 comittee is responsible for standardization within the area of Artificial Intelligence. There are 11 standards published and 37 standards under development under the direct responsibility of this technical committee. The major ISO/IEC standards related to AI are presented below. Foundational standards ISO/IEC 22989, Artificail Intelligence - Concepts and Terminology TBD (Andreas,just one sentence?) ISO/IEC 23053, Framework for Artificial Intelligence Systems Using Machine Learning TBD (Andreas) Trustworthyness ISO/IEC PRF TR 24368, Information technology \u2014 Artificial intelligence \u2014 Overview of ethical and societal concerns TBD (Andreas) ISO/IEC CD 23894 Information Technology \u2013 artificial Intelligence \u2013 risk management TBD (Andreas) ISO/IEC TR 24027:2021, Information technology \u2014 Artificial intelligence (AI) \u2014 Bias in AI systems and AI aided decision making TBD (Andreas) ISO/IEC TR 24028:2020, Information technology \u2013 Artificial intelligence \u2013 Overview of trustworthiness in artificial intelligence TBD (Meine) ISO/IEC TR 24029-1:2021, Artificial Intelligence (AI) \u2014 Assessment of the robustness of neural networks Neural Networks (NNs) are being widely used due to the promising results on various complex pattern learning tasks. Statistical analyses are used to measure the performance of NNs under varying conditions. Additionally, some form of formal analysis using formal methods and empirical analysis using empirical methods are also required to access the robustness of the NNs. In this regard, ISO/IEC TR 24029 is aimed at helping AI engineers and users to assess the robustness of NNs throughout their life cycle. More specifically, part 1 of the document helps understand the risks tied to the robustness of AI systems while part 2 focuses more on providing recommendations and requirements for the use of formal methods to assess the robustness. Overall, there are six steps in the assessment process. As the first step, robustness goals are stated . Generally, the goals are not only for the statistical analysis but also for the formal and empirical analysis. Then in the second and third steps, testing is planned and conducted . Once completed, the outcomes are analyzed in the fourth step, and the results are interpreted in the fifth step. Finally, in the last step, the decision on the system robustness is formulated* by comparing the interpreted results obtained in the fifth step and the robustness goals stated in the first step. Functional safety ISO/IEC AWI TR 5469, Artificial intelligence \u2014 Functional safety and AI systems (This technical report is under development) The purpose of ISO/IEC TR 5469 is to enable the developer of safety-related systems to appropriately apply AI technologies as part of safety functions by fostering awareness of the properties, safety risk (in the context of functional safety) factors, available methods and potential constraints of AI technologies. It does so by: Giving an overview of functional safety and its relevance for AI. Describing the use of AI technology in safety-related programmable systems. Providing a classification scheme for the applicability of AI in safety-related programmable systems. Explaining AI technology elements and the three-stage realization principle. Explaining properties of AI systems and how they relate to safety in the context of functional safety. Discussing verification and validation techniques. Describing control and mitigation measures. Showing how IEC 61508 3 can be interpreted for applying it to AI, and providing alternative ways for compliance when possible. Mapping AI life cycle models to IEC 61508-1. Governance implications ISO/IEC 38507:2022, Information technology \u2014 Governance of IT \u2014 Governance implications of the use of artificial intelligence by organizations The objective of this document is to provide guidance for the governing body of an organization that is using, or is considering the use of, artificial intelligence (AI). \u201cUse of AI\u201d is defined in the broadest sense as developing or applying an AI system through any part of its life cycle to fulfil objectives and create value for the organization. A governing body can consider deploying AI in order to pursue specific opportunities that the organization has identified. In such cases, the governing body needs to weigh those opportunities against risk and other implications of use. New implications that arise from the use of AI could include: * increased reliance on technology * transparency and explainability issues * differences in assumptions made when delegating tasks to humans vs AI * competitive pressure of an organization not using AI * unawareness of potential bias, errors or harms of embedding AI into existing complex systems * disparity in speed of change between automated learning systems human controls of compliance; * the impact of AI on the workforce * the impact of AI on commercial operations and to brand reputation. The governing body should take responsibility for the use of AI, rather than attributing responsibility to the AI system itself. Members of the governing body are responsible for informing themselves about the possibilities and risks raised by using AI systems. Members of the governing body are accountable for the use of AI considered acceptable by the organization. The use of AI can result in new obligations for the organization. These can be legal requirements or as a consequence of the adoption of voluntary codes of practice, whether directly within an AI system\u2019s automation of decision-making processes or indirectly through its use of data or other resources or processes. In section 5.5, the document describes actions that the organization may take to constrain the use of AI: - Increase oversight of compliance. - Address the scope of use. - Assess and address the impact on stakeholders. - Determine legal requirements or obligations of using such technology. - Align the use of AI to the objectives of the organization. - Align the use of AI to the organization\u2019s culture and values. - Ensure that problem solving takes due account of context. - Examine the additional risk that the use of AI can bring to the organization. Other ISO/IEC TR 24372:2021, Information technology \u2014 Artificial intelligence (AI) \u2014 Overview of computational approaches for AI systems ISO/IEC TR 24372 describes the typical characteristics of AI systems (e.g. adaptable, explainable, discriminative) and their computational characteristics (e.g. data-based, knowledge-base, infrastructure-based). It describes two categories of computational approaches: knowledge-driven and data-driven. It describes selected algorithms and approaches related to: knowledge engineering and representation, logic and reasoning, machine learning, and metaheuristics. ISO/IEC TR 24030:2021, Information technology \u2014 Artificial intelligence (AI) \u2014 Use cases ISO/IEC TR 24030 provides a collection of use cases of artificial intelligence (AI) applications in a variety of domains, such as: agriculture, digital marketing, education, energy, financial markets, healthcare, robotics, ICT, legal, logistics, manufacturing, public sector, security, and transportation. It aims to illustrate the applicability of the AI standardization work across a variety of application domains, and share the collected use cases in support of AI standardization work with external organizations and internal entities to foster collaboration. It also intends to help identify new technical requirements that may accelerate the development of AI technology.","title":"ISO/IEC"},{"location":"iso/#isoiec-standardization-of-ai","text":"The ISO/IEC JTC 1/SC 42 comittee is responsible for standardization within the area of Artificial Intelligence. There are 11 standards published and 37 standards under development under the direct responsibility of this technical committee. The major ISO/IEC standards related to AI are presented below.","title":"ISO/IEC standardization of AI"},{"location":"iso/#foundational-standards","text":"","title":"Foundational standards"},{"location":"iso/#isoiec-22989-artificail-intelligence-concepts-and-terminology","text":"TBD (Andreas,just one sentence?)","title":"ISO/IEC 22989, Artificail Intelligence - Concepts and Terminology"},{"location":"iso/#isoiec-23053-framework-for-artificial-intelligence-systems-using-machine-learning","text":"TBD (Andreas)","title":"ISO/IEC 23053, Framework for Artificial Intelligence Systems Using Machine Learning"},{"location":"iso/#trustworthyness","text":"","title":"Trustworthyness"},{"location":"iso/#isoiec-prf-tr-24368-information-technology-artificial-intelligence-overview-of-ethical-and-societal-concerns","text":"TBD (Andreas)","title":"ISO/IEC PRF TR 24368, Information technology \u2014 Artificial intelligence \u2014 Overview of ethical and societal concerns"},{"location":"iso/#isoiec-cd-23894-information-technology-artificial-intelligence-risk-management","text":"TBD (Andreas)","title":"ISO/IEC CD 23894 Information Technology \u2013 artificial Intelligence \u2013 risk management"},{"location":"iso/#isoiec-tr-240272021-information-technology-artificial-intelligence-ai-bias-in-ai-systems-and-ai-aided-decision-making","text":"TBD (Andreas)","title":"ISO/IEC TR 24027:2021, Information technology \u2014 Artificial intelligence (AI) \u2014 Bias in AI systems and AI aided decision making"},{"location":"iso/#isoiec-tr-240282020-information-technology-artificial-intelligence-overview-of-trustworthiness-in-artificial-intelligence","text":"TBD (Meine)","title":"ISO/IEC TR 24028:2020, Information technology \u2013 Artificial intelligence \u2013 Overview of trustworthiness in artificial intelligence"},{"location":"iso/#isoiec-tr-24029-12021-artificial-intelligence-ai-assessment-of-the-robustness-of-neural-networks","text":"Neural Networks (NNs) are being widely used due to the promising results on various complex pattern learning tasks. Statistical analyses are used to measure the performance of NNs under varying conditions. Additionally, some form of formal analysis using formal methods and empirical analysis using empirical methods are also required to access the robustness of the NNs. In this regard, ISO/IEC TR 24029 is aimed at helping AI engineers and users to assess the robustness of NNs throughout their life cycle. More specifically, part 1 of the document helps understand the risks tied to the robustness of AI systems while part 2 focuses more on providing recommendations and requirements for the use of formal methods to assess the robustness. Overall, there are six steps in the assessment process. As the first step, robustness goals are stated . Generally, the goals are not only for the statistical analysis but also for the formal and empirical analysis. Then in the second and third steps, testing is planned and conducted . Once completed, the outcomes are analyzed in the fourth step, and the results are interpreted in the fifth step. Finally, in the last step, the decision on the system robustness is formulated* by comparing the interpreted results obtained in the fifth step and the robustness goals stated in the first step.","title":"ISO/IEC TR 24029-1:2021, Artificial Intelligence (AI) \u2014 Assessment of the robustness of neural networks"},{"location":"iso/#functional-safety","text":"","title":"Functional safety"},{"location":"iso/#isoiec-awi-tr-5469-artificial-intelligence-functional-safety-and-ai-systems","text":"(This technical report is under development) The purpose of ISO/IEC TR 5469 is to enable the developer of safety-related systems to appropriately apply AI technologies as part of safety functions by fostering awareness of the properties, safety risk (in the context of functional safety) factors, available methods and potential constraints of AI technologies. It does so by: Giving an overview of functional safety and its relevance for AI. Describing the use of AI technology in safety-related programmable systems. Providing a classification scheme for the applicability of AI in safety-related programmable systems. Explaining AI technology elements and the three-stage realization principle. Explaining properties of AI systems and how they relate to safety in the context of functional safety. Discussing verification and validation techniques. Describing control and mitigation measures. Showing how IEC 61508 3 can be interpreted for applying it to AI, and providing alternative ways for compliance when possible. Mapping AI life cycle models to IEC 61508-1.","title":"ISO/IEC AWI TR 5469, Artificial intelligence \u2014 Functional safety and AI systems"},{"location":"iso/#governance-implications","text":"","title":"Governance implications"},{"location":"iso/#isoiec-385072022-information-technology-governance-of-it-governance-implications-of-the-use-of-artificial-intelligence-by-organizations","text":"The objective of this document is to provide guidance for the governing body of an organization that is using, or is considering the use of, artificial intelligence (AI). \u201cUse of AI\u201d is defined in the broadest sense as developing or applying an AI system through any part of its life cycle to fulfil objectives and create value for the organization. A governing body can consider deploying AI in order to pursue specific opportunities that the organization has identified. In such cases, the governing body needs to weigh those opportunities against risk and other implications of use. New implications that arise from the use of AI could include: * increased reliance on technology * transparency and explainability issues * differences in assumptions made when delegating tasks to humans vs AI * competitive pressure of an organization not using AI * unawareness of potential bias, errors or harms of embedding AI into existing complex systems * disparity in speed of change between automated learning systems human controls of compliance; * the impact of AI on the workforce * the impact of AI on commercial operations and to brand reputation. The governing body should take responsibility for the use of AI, rather than attributing responsibility to the AI system itself. Members of the governing body are responsible for informing themselves about the possibilities and risks raised by using AI systems. Members of the governing body are accountable for the use of AI considered acceptable by the organization. The use of AI can result in new obligations for the organization. These can be legal requirements or as a consequence of the adoption of voluntary codes of practice, whether directly within an AI system\u2019s automation of decision-making processes or indirectly through its use of data or other resources or processes. In section 5.5, the document describes actions that the organization may take to constrain the use of AI: - Increase oversight of compliance. - Address the scope of use. - Assess and address the impact on stakeholders. - Determine legal requirements or obligations of using such technology. - Align the use of AI to the objectives of the organization. - Align the use of AI to the organization\u2019s culture and values. - Ensure that problem solving takes due account of context. - Examine the additional risk that the use of AI can bring to the organization.","title":"ISO/IEC 38507:2022, Information technology \u2014 Governance of IT \u2014 Governance implications of the use of artificial intelligence by organizations"},{"location":"iso/#other","text":"","title":"Other"},{"location":"iso/#isoiec-tr-243722021-information-technology-artificial-intelligence-ai-overview-of-computational-approaches-for-ai-systems","text":"ISO/IEC TR 24372 describes the typical characteristics of AI systems (e.g. adaptable, explainable, discriminative) and their computational characteristics (e.g. data-based, knowledge-base, infrastructure-based). It describes two categories of computational approaches: knowledge-driven and data-driven. It describes selected algorithms and approaches related to: knowledge engineering and representation, logic and reasoning, machine learning, and metaheuristics.","title":"ISO/IEC TR 24372:2021, Information technology \u2014 Artificial intelligence (AI) \u2014 Overview of computational approaches for AI systems"},{"location":"iso/#isoiec-tr-240302021-information-technology-artificial-intelligence-ai-use-cases","text":"ISO/IEC TR 24030 provides a collection of use cases of artificial intelligence (AI) applications in a variety of domains, such as: agriculture, digital marketing, education, energy, financial markets, healthcare, robotics, ICT, legal, logistics, manufacturing, public sector, security, and transportation. It aims to illustrate the applicability of the AI standardization work across a variety of application domains, and share the collected use cases in support of AI standardization work with external organizations and internal entities to foster collaboration. It also intends to help identify new technical requirements that may accelerate the development of AI technology.","title":"ISO/IEC TR 24030:2021, Information technology \u2014 Artificial intelligence (AI) \u2014 Use cases"},{"location":"nz/","text":"Regulation of AI in New Zealand The approach of the New Zealand Government is to progressively incorporate regulation of AI into existing legislation and regulations, rather than developing new AI-specific regulations and legislation. New Zealand has an ICT Operations Assurance Framework . They define assurance as \"an independent and objective assessment that provides credible information to support decision-making\", and list six principles of good assurance: * Assurance by design * Flexible * Informs key decisions * Risk and outcomes based * Independent and impartial * Accountability The New Zealand government also provide guidance and templates for ICT operations assurance , consisting of the following steps: 1. Managing your ICT (Information and Communications Technology) risks 2. Developing your assurance plan 3. Maximizing the value of independent assurance 4. Ensuring high quality assurance information 5. Overseeing assurance activities and recommendations 6. Capturing lessons learned","title":"New Zealand"},{"location":"nz/#regulation-of-ai-in-new-zealand","text":"The approach of the New Zealand Government is to progressively incorporate regulation of AI into existing legislation and regulations, rather than developing new AI-specific regulations and legislation. New Zealand has an ICT Operations Assurance Framework . They define assurance as \"an independent and objective assessment that provides credible information to support decision-making\", and list six principles of good assurance: * Assurance by design * Flexible * Informs key decisions * Risk and outcomes based * Independent and impartial * Accountability The New Zealand government also provide guidance and templates for ICT operations assurance , consisting of the following steps: 1. Managing your ICT (Information and Communications Technology) risks 2. Developing your assurance plan 3. Maximizing the value of independent assurance 4. Ensuring high quality assurance information 5. Overseeing assurance activities and recommendations 6. Capturing lessons learned","title":"Regulation of AI in New Zealand"},{"location":"oecd/","text":"TBD","title":"OECD"},{"location":"uk/","text":"Regulation of AI in the United Kingdom In 2021, the UK Centre for Data Ethics and Innovaion (CDEI) published a roadmap , which is the first of its kind, setting out the steps required to build a world-leading AI assurance ecosystem in the UK. Their vision is that the UK will have a thriving and effective AI assurance ecosystem within the next 5 years. To provide meaningful and reliable assurance for AI, organisations need to overcome: * An information problem: reliably evaluate evidence to assess whether an AI system is trustworthy. * A communication problem: communicate the evidence at the right level, to inform assurance users\u2019 views on whether to trust an AI system. Assurance helps to overcome both of these problems. The UK roadmap describes three roles in relation to AI assurance, i.e., the 1st party role (responsible party), the 2nd party role (assurance user) and the 3rd party role (assurance provider). The 3rd party conducts assessment, testing and verification of the AI -system for the first party, and provides information about trustworthiness of the AI-system to the 2nd party. This enables the 2nd party to have justified trust in the 1at party. In addition, the roadmap describes four sets of key actors in the Ai assurance ecosystem: * Simplified AI supply chain: AI developers, executives deploying AI systems, Frontline users and affected individuals. * AI assurance service roviders: Independent assurance providers and internal assurance teams. * Independent research: Academic researchers and journalists/activists. * Supporting structures: Government, regulators, standards bodies and accreditation/professional bodies. Based on the main roles of these four important groups of actors in the AI assurance ecosystem, the CDEI have identified six priority areas for developing an effective, mature AI assurance ecosystem: Demand for AI assurance: The AI supply chain will need to demand, and receive, reliable evidence about the risks of these technologies, so they can make responsible adoption decisions. An AI assurance market: A competitive, dynamic market of service providers is needed to provide the tools and services to create this reliable evidence in an efficient and effective way. Standards: Different kinds of standards are needed to build common language and scalable assessment techniques Professionalization: The governance and incentives for assurance service providers need to be trusted. Regulation: Beyond auditing and inspecting AI as part of their enforcement activity, regulators will play an important role in supporting the development of the broader AI assurance ecosystem. Regulation can help to enable assurance, by setting assurable requirements that enable organizations to manage their regulatory obligations. Assurance also helps regulators to achieve their objectives by empowering users of AI to achieve compliance and manage risk. Independent research: In addition to specialized assurance providers, standards, regulatory, industry and professional bodies, other independent actors can offer important services to the assurance ecosystem.","title":"UK"},{"location":"uk/#regulation-of-ai-in-the-united-kingdom","text":"In 2021, the UK Centre for Data Ethics and Innovaion (CDEI) published a roadmap , which is the first of its kind, setting out the steps required to build a world-leading AI assurance ecosystem in the UK. Their vision is that the UK will have a thriving and effective AI assurance ecosystem within the next 5 years. To provide meaningful and reliable assurance for AI, organisations need to overcome: * An information problem: reliably evaluate evidence to assess whether an AI system is trustworthy. * A communication problem: communicate the evidence at the right level, to inform assurance users\u2019 views on whether to trust an AI system. Assurance helps to overcome both of these problems. The UK roadmap describes three roles in relation to AI assurance, i.e., the 1st party role (responsible party), the 2nd party role (assurance user) and the 3rd party role (assurance provider). The 3rd party conducts assessment, testing and verification of the AI -system for the first party, and provides information about trustworthiness of the AI-system to the 2nd party. This enables the 2nd party to have justified trust in the 1at party. In addition, the roadmap describes four sets of key actors in the Ai assurance ecosystem: * Simplified AI supply chain: AI developers, executives deploying AI systems, Frontline users and affected individuals. * AI assurance service roviders: Independent assurance providers and internal assurance teams. * Independent research: Academic researchers and journalists/activists. * Supporting structures: Government, regulators, standards bodies and accreditation/professional bodies. Based on the main roles of these four important groups of actors in the AI assurance ecosystem, the CDEI have identified six priority areas for developing an effective, mature AI assurance ecosystem: Demand for AI assurance: The AI supply chain will need to demand, and receive, reliable evidence about the risks of these technologies, so they can make responsible adoption decisions. An AI assurance market: A competitive, dynamic market of service providers is needed to provide the tools and services to create this reliable evidence in an efficient and effective way. Standards: Different kinds of standards are needed to build common language and scalable assessment techniques Professionalization: The governance and incentives for assurance service providers need to be trusted. Regulation: Beyond auditing and inspecting AI as part of their enforcement activity, regulators will play an important role in supporting the development of the broader AI assurance ecosystem. Regulation can help to enable assurance, by setting assurable requirements that enable organizations to manage their regulatory obligations. Assurance also helps regulators to achieve their objectives by empowering users of AI to achieve compliance and manage risk. Independent research: In addition to specialized assurance providers, standards, regulatory, industry and professional bodies, other independent actors can offer important services to the assurance ecosystem.","title":"Regulation of AI in the United Kingdom"}]}