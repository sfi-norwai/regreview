{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"AI regulation and governance This section contains resources about the regulation and governance of artificial intelligence (AI). It covers policies in various jusrisdictions, as well as standards, guidelines, recommended practices and frameworks for the promotion and assurance of trustworthy AI. The contents are organised in the following sections: EU: This section summarises the proposed EU AI act. China: This section describes the principles for AI governance and standardisation framework in China. UK: This section descibes the UK roadmap to an effective AI assurance ecosystem. New Zealand: This section describes thye principles approach of the New Zealand government to AI governance. ISO/IEC: This section describes important AI related ISO/IEC standards that are published or under development. IEEE: TBD OECD: TBD DNV: TBD Definitions: This section dicuss important definitions related to AI and assurance of AI.","title":"Introduction"},{"location":"#ai-regulation-and-governance","text":"This section contains resources about the regulation and governance of artificial intelligence (AI). It covers policies in various jusrisdictions, as well as standards, guidelines, recommended practices and frameworks for the promotion and assurance of trustworthy AI. The contents are organised in the following sections: EU: This section summarises the proposed EU AI act. China: This section describes the principles for AI governance and standardisation framework in China. UK: This section descibes the UK roadmap to an effective AI assurance ecosystem. New Zealand: This section describes thye principles approach of the New Zealand government to AI governance. ISO/IEC: This section describes important AI related ISO/IEC standards that are published or under development. IEEE: TBD OECD: TBD DNV: TBD Definitions: This section dicuss important definitions related to AI and assurance of AI.","title":"AI regulation and governance"},{"location":"china/","text":"AI governance and standardization in China 2018 White Paper In 2018, the Chinese government-issued AI white paper highlights the key factors that should be considered in the standards-setting for AI. Before it starts explaining its framework for standards-setting, the white paper introduces the concept of AI, the current situation and trends, and safety, ethical, and privacy issues related to AI. After summarizing the existing AI standards and regulations, both locally and internationally, their structure of the AI standardization system is elucidated. The structure consists of six parts: foundation, platforms/support, key technology, products and services, applications, and security/ethics. These can be briefly elaborated as follows. Foundational standards target mainly regularizing fundamentals of AI such as terminology, reference architecture, data, and testing and assessment. The main focuses of the platforms/support component of these standards are big data, cloud computing, intelligent perception and connection, edge computing, smart chips, and AI platforms. Once the data is prepared according to foundational standards and necessary supportive technologies are gathered as per platforms/support standards, AI-related technologies should be used in accordance with key technical standards to perform humanlike learning from data. The intelligent systems made by following the above standards are then embedded into products and used in services, however, should be done in obedience to products and services standards . While using these products and services in different application domains, applications standards should be followed. Their standards system framework names smart manufacturing, smart logistics, smart cities, smart homes, smart transportation, smart finance, and smart healthcare as potential example applications. Throughout the entire process from data garnering to final employment of AI products/services in different applications, developers and users should comply with ethical and moral dimensions and uphold societal security and human rights as per security/ethics standards . Eight principles for AI governance and \u201cresponsible AI\" In 2019, China's New Generation AI Governance Expert Committee published \"Eight principles for AI governance and \u201cresponsible AI\u201d . The motivation behind the proposed principles is the healthy development of a new generation of AI. If followed, one can ensure that AI is safe/secure, and reliable. The eight principles are as follows: Harmony and friendliness: The primary objective of AI should be to enhance the common well-being of humanity. In other words, AI should serve the progress of human civilization while complying with the ethical and moral dimensions, and upholding the societal security and human rights. Fairness and justice: Biasness and discrimination should be avoided in each phase from data gathering to the final product development of AI. Instead, fairness, justice, and equality of opportunity should be promoted. The rights and interests of stakeholders should be protected. Inclusivity and sharing: AI should be for everyone. Coordinated, shared, and inclusive development of AI should be encouraged to secure the adaptability of people from different backgrounds and abilities. Resources should be openly accessible to evade data and platform monopolies. Respect privacy: In the development of AI, personal privacy should be strictly protected. If personal data is used, necessary boundaries and standards should be enforced to protect them. Contributing individuals should have the right to know and decide which data should be used and where they should be used. Secure/safe and controllable: In order to attain trustworthiness, AI systems should emphasize transparency, explainability, reliability, and controllability. The robustness, safety, and security of AI systems must be given special attention, and an external assessment of these components is needed. Shared responsibility: In general, it is the responsibility of AI developers, users, beneficiaries, and all the stakeholders involved to prevent the misuse of AI against laws, regulations, ethics, morals, standards, and norms. When needed, the specific responsibilities of these parties should also be defined. Open collaboration: To promote the healthy development of AI, knowledge and ideas should be shared and exchanged across disciplines, domains, regions, and borders. For collaborations at the international level, consensus on an international AI governance framework, standards, and norms should be initiated. Agile governance: In order to promote the innovative, orderly, and natural development of AI, management mechanisms, governance systems, and recommender practices should be continuously updated. Future risks associated with the advancements in AI should be anticipated and necessary adjustments should be made accordingly. 2021 White Paper In 2021, China published a white paper on Trusted artificial intelligence . It highlights the need for regulations and law to guide AI development, and also describes the use of 3rd parties for evaluations and verifications. It is suggested that AI enterprises and insurance institutions can explore the insurance mechanism of AI product application, conduct quantitative assessment of risk accidents, provide risk compensation, and help improve the trusted AI ecosystem.","title":"China"},{"location":"china/#ai-governance-and-standardization-in-china","text":"","title":"AI governance and standardization in China"},{"location":"china/#2018-white-paper","text":"In 2018, the Chinese government-issued AI white paper highlights the key factors that should be considered in the standards-setting for AI. Before it starts explaining its framework for standards-setting, the white paper introduces the concept of AI, the current situation and trends, and safety, ethical, and privacy issues related to AI. After summarizing the existing AI standards and regulations, both locally and internationally, their structure of the AI standardization system is elucidated. The structure consists of six parts: foundation, platforms/support, key technology, products and services, applications, and security/ethics. These can be briefly elaborated as follows. Foundational standards target mainly regularizing fundamentals of AI such as terminology, reference architecture, data, and testing and assessment. The main focuses of the platforms/support component of these standards are big data, cloud computing, intelligent perception and connection, edge computing, smart chips, and AI platforms. Once the data is prepared according to foundational standards and necessary supportive technologies are gathered as per platforms/support standards, AI-related technologies should be used in accordance with key technical standards to perform humanlike learning from data. The intelligent systems made by following the above standards are then embedded into products and used in services, however, should be done in obedience to products and services standards . While using these products and services in different application domains, applications standards should be followed. Their standards system framework names smart manufacturing, smart logistics, smart cities, smart homes, smart transportation, smart finance, and smart healthcare as potential example applications. Throughout the entire process from data garnering to final employment of AI products/services in different applications, developers and users should comply with ethical and moral dimensions and uphold societal security and human rights as per security/ethics standards .","title":"2018 White Paper"},{"location":"china/#eight-principles-for-ai-governance-and-responsible-ai","text":"In 2019, China's New Generation AI Governance Expert Committee published \"Eight principles for AI governance and \u201cresponsible AI\u201d . The motivation behind the proposed principles is the healthy development of a new generation of AI. If followed, one can ensure that AI is safe/secure, and reliable. The eight principles are as follows: Harmony and friendliness: The primary objective of AI should be to enhance the common well-being of humanity. In other words, AI should serve the progress of human civilization while complying with the ethical and moral dimensions, and upholding the societal security and human rights. Fairness and justice: Biasness and discrimination should be avoided in each phase from data gathering to the final product development of AI. Instead, fairness, justice, and equality of opportunity should be promoted. The rights and interests of stakeholders should be protected. Inclusivity and sharing: AI should be for everyone. Coordinated, shared, and inclusive development of AI should be encouraged to secure the adaptability of people from different backgrounds and abilities. Resources should be openly accessible to evade data and platform monopolies. Respect privacy: In the development of AI, personal privacy should be strictly protected. If personal data is used, necessary boundaries and standards should be enforced to protect them. Contributing individuals should have the right to know and decide which data should be used and where they should be used. Secure/safe and controllable: In order to attain trustworthiness, AI systems should emphasize transparency, explainability, reliability, and controllability. The robustness, safety, and security of AI systems must be given special attention, and an external assessment of these components is needed. Shared responsibility: In general, it is the responsibility of AI developers, users, beneficiaries, and all the stakeholders involved to prevent the misuse of AI against laws, regulations, ethics, morals, standards, and norms. When needed, the specific responsibilities of these parties should also be defined. Open collaboration: To promote the healthy development of AI, knowledge and ideas should be shared and exchanged across disciplines, domains, regions, and borders. For collaborations at the international level, consensus on an international AI governance framework, standards, and norms should be initiated. Agile governance: In order to promote the innovative, orderly, and natural development of AI, management mechanisms, governance systems, and recommender practices should be continuously updated. Future risks associated with the advancements in AI should be anticipated and necessary adjustments should be made accordingly.","title":"Eight principles for AI governance and \u201cresponsible AI\""},{"location":"china/#2021-white-paper","text":"In 2021, China published a white paper on Trusted artificial intelligence . It highlights the need for regulations and law to guide AI development, and also describes the use of 3rd parties for evaluations and verifications. It is suggested that AI enterprises and insurance institutions can explore the insurance mechanism of AI product application, conduct quantitative assessment of risk accidents, provide risk compensation, and help improve the trusted AI ecosystem.","title":"2021 White Paper"},{"location":"definitions/","text":"TBD","title":"Definitions"},{"location":"dnv/","text":"TBD","title":"DNV"},{"location":"eu/","text":"AI governance and regulation in the European union The EU\u2019s approach to artificial intelligence centres on excellence and trust, aiming to boost research and industrial capacity and ensure fundamental rights. The European Commission has proposed three inter-related legal initiatives that will contribute to building trustworthy AI: A European legal framework for AI to address fundamental rights and safety risks specific to the AI systems; EU rules to address liability issues related to new technologies, including AI systems; A revision of sectoral safety legislation (e.g. Machinery Regulation, General Product Safety Directive). High-Level Expert Group on AI \u2013 Ethics Guidelines for trustworthy AI The European Commission\u2019s High-Level Expert Group on AI published Ethics Guidelines for trustworthy AI in 2019. It introduces three necessary (but not sufficient) components for achievement of trustworthy AI, that should be met throughout the AI system's entire life cycle, namely that it should be lawful , ethical and robust . The guideline present a framework addressing the ethics and robustness aspects. Chapter I (Foundations of Trustworthy AI) identifies the ethical principles and their correlated values that must be respected in the development, deployment and use of AI systems. Key ethical principles include respect for human autonomy , prevention of harm , fairness and explicability . Chapter II (Realising Trustworthy AI) provides guidance on how Trustworthy AI can be realised, by listing seven requirements that AI systems should meet: human agency and oversight; technical robustness and safety; privacy and data governance; transparency; diversity, non-discrimination and fairness; environmental and societal well-being, and; accountability. Chapter III (Assessing Trustworthy AI) provides a concrete and non-exhaustive Trustworthy AI assessment list aimed at operationalising the key requirements set out in Chapter II. The EU AI Act The AI act proposed by the European Commission in April 2021 is the world\u2019s first legal framework for AI, with potential approval in 2022 and potential entry-into-force in 2023. After passing the act there will be a two-year implementation period. Systems existing at the time of implementation are exempted from the requirements in the act unless they subsequently experience a significant change in purpose or design. The AI act will apply \u201cwhere the output produced by the system is used in the Union\u201d - irrespective of where the provider and user are located. The regulation will not apply to AI systems developed and used solely for scientific research, as long as such activities do not lead to placing an AI system on the market or into service in the EU. The proposed AI act is risk based and classifies AI systems into four risk categories, and a screening is required to determine the level of regulation: Unacceptable risk: These are prohibited. And include social scoring systems, subliminal techniques for manipulation, exploitation of children or mentally disabled, etc. High risk: These are permitted subject to compliance with AI requirements and conformity assessments. Limited risk: These are permitted, but subject to information/transparency requirements. Minimal/no risk: These are permitted with no restrictions, and application of AI requirements are voluntary. Two categories of high-risk AI are defined: Safety components of regulated products , which are subject to third-party assessment under the relevant sectorial legislation. Stand-alone AI systems in 8 different fields, namely: Biometric identification of natural persons Management and operation of critical infrastructure Educational and vocational training (to determine access, assessment, admission) Employment, workers management and access to self-employment Access to and enjoyment of essential private services and public services and benefits Law enforcement Migration, asylum and border control management Administration of justice and democratic processes For high risk AI there are requirements to the management system (article 9), data and data governance (article 10), technical documentation (article 11), record keeping (article 12) transparency (article 13), human oversight (article 14) and accuracy, robustness and cybersecurity (article 15). A conformity assessment is required for high-risk AI systems. For seven of the eight mentioned high-risk AI systems, an internal conformity assessment is permitted. For high-risk AI systems that relate to the biometric identification and categorization of natural persons, the provider may choose internal conformity assessment only if harmonised standards or common specifications have been applied - otherwise the conformity assessment must be conducted by a Notified Body (third party assurance provider).","title":"EU"},{"location":"eu/#ai-governance-and-regulation-in-the-european-union","text":"The EU\u2019s approach to artificial intelligence centres on excellence and trust, aiming to boost research and industrial capacity and ensure fundamental rights. The European Commission has proposed three inter-related legal initiatives that will contribute to building trustworthy AI: A European legal framework for AI to address fundamental rights and safety risks specific to the AI systems; EU rules to address liability issues related to new technologies, including AI systems; A revision of sectoral safety legislation (e.g. Machinery Regulation, General Product Safety Directive).","title":"AI governance and regulation in the European union"},{"location":"eu/#high-level-expert-group-on-ai-ethics-guidelines-for-trustworthy-ai","text":"The European Commission\u2019s High-Level Expert Group on AI published Ethics Guidelines for trustworthy AI in 2019. It introduces three necessary (but not sufficient) components for achievement of trustworthy AI, that should be met throughout the AI system's entire life cycle, namely that it should be lawful , ethical and robust . The guideline present a framework addressing the ethics and robustness aspects. Chapter I (Foundations of Trustworthy AI) identifies the ethical principles and their correlated values that must be respected in the development, deployment and use of AI systems. Key ethical principles include respect for human autonomy , prevention of harm , fairness and explicability . Chapter II (Realising Trustworthy AI) provides guidance on how Trustworthy AI can be realised, by listing seven requirements that AI systems should meet: human agency and oversight; technical robustness and safety; privacy and data governance; transparency; diversity, non-discrimination and fairness; environmental and societal well-being, and; accountability. Chapter III (Assessing Trustworthy AI) provides a concrete and non-exhaustive Trustworthy AI assessment list aimed at operationalising the key requirements set out in Chapter II.","title":"High-Level Expert Group on AI \u2013 Ethics Guidelines for trustworthy AI"},{"location":"eu/#the-eu-ai-act","text":"The AI act proposed by the European Commission in April 2021 is the world\u2019s first legal framework for AI, with potential approval in 2022 and potential entry-into-force in 2023. After passing the act there will be a two-year implementation period. Systems existing at the time of implementation are exempted from the requirements in the act unless they subsequently experience a significant change in purpose or design. The AI act will apply \u201cwhere the output produced by the system is used in the Union\u201d - irrespective of where the provider and user are located. The regulation will not apply to AI systems developed and used solely for scientific research, as long as such activities do not lead to placing an AI system on the market or into service in the EU. The proposed AI act is risk based and classifies AI systems into four risk categories, and a screening is required to determine the level of regulation: Unacceptable risk: These are prohibited. And include social scoring systems, subliminal techniques for manipulation, exploitation of children or mentally disabled, etc. High risk: These are permitted subject to compliance with AI requirements and conformity assessments. Limited risk: These are permitted, but subject to information/transparency requirements. Minimal/no risk: These are permitted with no restrictions, and application of AI requirements are voluntary. Two categories of high-risk AI are defined: Safety components of regulated products , which are subject to third-party assessment under the relevant sectorial legislation. Stand-alone AI systems in 8 different fields, namely: Biometric identification of natural persons Management and operation of critical infrastructure Educational and vocational training (to determine access, assessment, admission) Employment, workers management and access to self-employment Access to and enjoyment of essential private services and public services and benefits Law enforcement Migration, asylum and border control management Administration of justice and democratic processes For high risk AI there are requirements to the management system (article 9), data and data governance (article 10), technical documentation (article 11), record keeping (article 12) transparency (article 13), human oversight (article 14) and accuracy, robustness and cybersecurity (article 15). A conformity assessment is required for high-risk AI systems. For seven of the eight mentioned high-risk AI systems, an internal conformity assessment is permitted. For high-risk AI systems that relate to the biometric identification and categorization of natural persons, the provider may choose internal conformity assessment only if harmonised standards or common specifications have been applied - otherwise the conformity assessment must be conducted by a Notified Body (third party assurance provider).","title":"The EU AI Act"},{"location":"ieee/","text":"IEEE standards on AI Ethically aligned design (Version II) Ethically Aligned Design paperwork, published under the auspices of the IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems, highlights the ethical considerations in the design of autonomous and intelligent systems (A/IS). The document contains eleven chapters, and each chapter accents the following aspects. Ch. 1 - From Principles to Practice: In this chapter, first, they introduce the three main areas which their framework expands into, namely, universal human values, political self-determination and data agency, and technical dependability. Then they connect these with the high-level general principles introduced in the second chapter. Ch. 2 - General Principles: Chapter two details the general principles: human rights, well-being, data agency, effectiveness, transparency, accountability, awareness of misuse, and competence. These general principles guide all manner of ethical A/IS design. Ch. 3 - Classical Ethics in A/IS: In the current digital age, human control of AI tools in the deployment phase has been reduced. Hence, the creator of such tools should ask themselves how culturally and ethically sound the design of these tools are before they are implemented. This is the emphasis of chapter three where they bring two thousand years\u2019 worth of classical ethics traditions into consideration. Ch. 4 - Well-being: The current measures of success of AI tools such as profit, gross domestic product (GDP), consumption levels, and occupational safety do not encompass the well-being of humans and society. This raises the issue that AI developers overlook innovation toward well-being and societal value. Chapter four makes AI developers aware of these issues and makes them consider the well-being aspect while designing AI tools. Ch. 5 - Affective Computing: During the interaction between humans and AI, AI systems could simulate emotions in humans. This human emotional experience due to AI systems has the potential to change human life, and eventually society, both positively and negatively. Chapter five addresses issues related to emotions and emotion-like control in interactions between humans and the design of A/IS. Ch. 6 - Personal Data and Individual Agency: Chapter six discusses the unpopular aspect of data protection, i.e., algorithms learned from personal behavior influence the individuals\u2019 choices and shape their life trajectory. Hence, governments and organizations should institute regulations as to who can process what personal data for what purposes. Ch. 7 - Methods to Guide Ethical Research and Design: Chapter seven contains three sections: Interdisciplinary Education and Research, Practices on A/IS, and Responsibility and Assessment. The aim of the chapter is to avail the researchers, product developers, and technologists to research and develop methods that evaluate their processes, products, values, and design practices in light of ethics. Ch. 8 - A/IS for Sustainable Development: If operated aptly, AI is a great opportunity for high-income and low-and middle-income countries to attain positive socioeconomic outcomes and sustainable development goals. Chapter eight provides an insight into how AI should be harnessed to achieve sustainable development. Ch. 9 - Embedding Values into Autonomous and Intelligent Systems: Although it is essential that AI systems adapt, learn, and follow the norms and values of the community they serve, current AI standards and principles do not review embedding human values and norms into AI. This chapter helps identify, implement, and evaluate values that should be embedded into AI. Ch. 10 \u2013 Policy: AI policies and regulations should be developed to protect and promote safety, privacy, human rights, and cybersecurity, as well as enhance the public\u2019s understanding of the potential impacts of A/IS on society. Otherwise, there may be critical technology failures, loss of life, and high-profile social controversies. Chapter ten presents a rights-based approach for the policymakers to build AI policies and regulations. Ch. 11 \u2013 Law: Technological innovation caused by AI should be guided by laws. On the other hand, the law should respond to technological innovation due to AI. This complex interactive process should eventually ensure that A/IS, in both design and operation, is aligned with principles of ethics and human well-being. This is the focus of the last chapter of this document. 7010-2020: IEEE Recommended Practice for Assessing the Impact of Autonomous and Intelligent Systems on Human Well-Being The aim of the document is to provide recommended practices for AI developers to increase and help safeguard human well-being at the individual, population, and societal levels. The document specifies specific and contextual well-being metrics that facilitate the use of the Well-Being Impact Assessment (WIA) process. WIA is an iterative procedure that should be repeated throughout the AI lifecycle. The WIA is composed of five activities, as follows: Activity 1 (Internal, User, and Stakeholder Impact Assessment) involves first identifying and later refining well-being domains and indicators. Activity 2 (Well-Being Indicators Dashboard) helps to form the well-being indicator dashboard based on the domains and indicators selected in Activity 1. Activity 3 (Data Collection Plan and Data Collection) populates the well-being indicator dashboard with data. This involves both planning and collecting data from users and stakeholders. In Activity 4 (Well-Being Data Analysis and Use of Well-Being Data) , the well-being data is analyzed and used for the design, development, deployment, monitoring, and iterative improvement of the AI tool. Activity 5 (Iterate) involves assessing the WIA process and well-being indicator dashboard. Changes necessary to be made for the next round are marked and continue. If one follows successfully, One will develop AI with well-being concepts and indicators in mind. One will be able to monitor, evaluate, and address AI impact on human well-being. One will get the ability to evaluate the ongoing well-being impacts of AI. One will be able to use acquired knowledge to help the continuation of safeguarding and improving human well-being. One will gain the ability to avoid unintentionally harming the well-being of users.","title":"IEEE"},{"location":"ieee/#ieee-standards-on-ai","text":"","title":"IEEE standards on AI"},{"location":"ieee/#ethically-aligned-design-version-ii","text":"Ethically Aligned Design paperwork, published under the auspices of the IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems, highlights the ethical considerations in the design of autonomous and intelligent systems (A/IS). The document contains eleven chapters, and each chapter accents the following aspects. Ch. 1 - From Principles to Practice: In this chapter, first, they introduce the three main areas which their framework expands into, namely, universal human values, political self-determination and data agency, and technical dependability. Then they connect these with the high-level general principles introduced in the second chapter. Ch. 2 - General Principles: Chapter two details the general principles: human rights, well-being, data agency, effectiveness, transparency, accountability, awareness of misuse, and competence. These general principles guide all manner of ethical A/IS design. Ch. 3 - Classical Ethics in A/IS: In the current digital age, human control of AI tools in the deployment phase has been reduced. Hence, the creator of such tools should ask themselves how culturally and ethically sound the design of these tools are before they are implemented. This is the emphasis of chapter three where they bring two thousand years\u2019 worth of classical ethics traditions into consideration. Ch. 4 - Well-being: The current measures of success of AI tools such as profit, gross domestic product (GDP), consumption levels, and occupational safety do not encompass the well-being of humans and society. This raises the issue that AI developers overlook innovation toward well-being and societal value. Chapter four makes AI developers aware of these issues and makes them consider the well-being aspect while designing AI tools. Ch. 5 - Affective Computing: During the interaction between humans and AI, AI systems could simulate emotions in humans. This human emotional experience due to AI systems has the potential to change human life, and eventually society, both positively and negatively. Chapter five addresses issues related to emotions and emotion-like control in interactions between humans and the design of A/IS. Ch. 6 - Personal Data and Individual Agency: Chapter six discusses the unpopular aspect of data protection, i.e., algorithms learned from personal behavior influence the individuals\u2019 choices and shape their life trajectory. Hence, governments and organizations should institute regulations as to who can process what personal data for what purposes. Ch. 7 - Methods to Guide Ethical Research and Design: Chapter seven contains three sections: Interdisciplinary Education and Research, Practices on A/IS, and Responsibility and Assessment. The aim of the chapter is to avail the researchers, product developers, and technologists to research and develop methods that evaluate their processes, products, values, and design practices in light of ethics. Ch. 8 - A/IS for Sustainable Development: If operated aptly, AI is a great opportunity for high-income and low-and middle-income countries to attain positive socioeconomic outcomes and sustainable development goals. Chapter eight provides an insight into how AI should be harnessed to achieve sustainable development. Ch. 9 - Embedding Values into Autonomous and Intelligent Systems: Although it is essential that AI systems adapt, learn, and follow the norms and values of the community they serve, current AI standards and principles do not review embedding human values and norms into AI. This chapter helps identify, implement, and evaluate values that should be embedded into AI. Ch. 10 \u2013 Policy: AI policies and regulations should be developed to protect and promote safety, privacy, human rights, and cybersecurity, as well as enhance the public\u2019s understanding of the potential impacts of A/IS on society. Otherwise, there may be critical technology failures, loss of life, and high-profile social controversies. Chapter ten presents a rights-based approach for the policymakers to build AI policies and regulations. Ch. 11 \u2013 Law: Technological innovation caused by AI should be guided by laws. On the other hand, the law should respond to technological innovation due to AI. This complex interactive process should eventually ensure that A/IS, in both design and operation, is aligned with principles of ethics and human well-being. This is the focus of the last chapter of this document.","title":"Ethically aligned design (Version II)"},{"location":"ieee/#7010-2020-ieee-recommended-practice-for-assessing-the-impact-of-autonomous-and-intelligent-systems-on-human-well-being","text":"The aim of the document is to provide recommended practices for AI developers to increase and help safeguard human well-being at the individual, population, and societal levels. The document specifies specific and contextual well-being metrics that facilitate the use of the Well-Being Impact Assessment (WIA) process. WIA is an iterative procedure that should be repeated throughout the AI lifecycle. The WIA is composed of five activities, as follows: Activity 1 (Internal, User, and Stakeholder Impact Assessment) involves first identifying and later refining well-being domains and indicators. Activity 2 (Well-Being Indicators Dashboard) helps to form the well-being indicator dashboard based on the domains and indicators selected in Activity 1. Activity 3 (Data Collection Plan and Data Collection) populates the well-being indicator dashboard with data. This involves both planning and collecting data from users and stakeholders. In Activity 4 (Well-Being Data Analysis and Use of Well-Being Data) , the well-being data is analyzed and used for the design, development, deployment, monitoring, and iterative improvement of the AI tool. Activity 5 (Iterate) involves assessing the WIA process and well-being indicator dashboard. Changes necessary to be made for the next round are marked and continue. If one follows successfully, One will develop AI with well-being concepts and indicators in mind. One will be able to monitor, evaluate, and address AI impact on human well-being. One will get the ability to evaluate the ongoing well-being impacts of AI. One will be able to use acquired knowledge to help the continuation of safeguarding and improving human well-being. One will gain the ability to avoid unintentionally harming the well-being of users.","title":"7010-2020: IEEE Recommended Practice for Assessing the Impact of Autonomous and Intelligent Systems on Human Well-Being"},{"location":"iso/","text":"ISO/IEC standardization of AI The ISO/IEC JTC 1/SC 42 comittee is responsible for standardization within the area of Artificial Intelligence. There are 11 standards published and 37 standards under development under the direct responsibility of this technical committee. The major ISO/IEC standards related to AI are presented below. Foundational standards ISO/IEC 22989, Artificail Intelligence - Concepts and Terminology This document provides standardized concepts and terminology to help artificial intelligence technology be better understood and adopted by a broad set of stakeholders. This document can be used in the development of other standards and in support of communications among diverse, interested parties/stakeholders. ISO/IEC 23053, Framework for Artificial Intelligence Systems Using Machine Learning This document aims to provide a framework for the description of AI systems that use ML. By establishing a common terminology and a common set of concepts for such systems, this document provides a basis for clear explanation of the systems and various considerations that apply to their engineering and to their use. The document describes machine learning systems (chapter 6), machine learning approaches (chapter 7), The machine learning pipeline (chapter 8) and the machine learning process (chapter 9). Annex A maps ISO/IEC 23053 data types to ISO/IEC 19944-1 data categories. Trustworthyness ISO/IEC PRF TR 24368, Information technology \u2014 Artificial intelligence \u2014 Overview of ethical and societal concerns This document provides a high-level overview of AI ethical and societal concerns. Examples of ethical and societal concerns in AI include privacy and security breaches to discriminatory outcomes and impact on human autonomy. The values embedded in algorithms, as well as the choice of problems AI systems and applications are used for to address, can be intentionally or inadvertently shaped by developers\u2019 and stakeholders\u2019 own worldviews and cognitive bias. Ethical and social concerns may arise even if an AI system performs flawlessly from a technical perspective. The document describes ethical frameworks, human rights practices, and principles for responsible use of AI. Appendix A is informative and gives an overview and summary of principles for ethical AI from around the world. Appendix B is informative and elaborates on ethical and societal concerns in several use cases. ISO/IEC CD 23894 Information Technology \u2013 artificial Intelligence \u2013 risk management AI systems can introduce new or emergent risks for an organization, with positive or negative consequences on objectives, or changes in the likelihood of existing risks. This document provides guidelines on how organizations that develop, produce, deploy or use products, systems and services that utilize AI can manage risk specifically related to AI. It uses ISO 31000:2018 as a normative reference and is intended to be used in connection with ISO 31000:2018. The clause structure of ISO 31000:2018 is mirrored in this document and amended by sub-clauses if needed. The main parts in the document are Clause 5 (principles), clause 5 (framework) and clause 6 (processes). Common AI-related objectives and risk sources are provided in Annex A and Annex B. Annex C provides an example mapping between the risk management processes and an AI system life cycle. ISO/IEC TR 24027:2021, Information technology \u2014 Artificial intelligence (AI) \u2014 Bias in AI systems and AI aided decision making Bias in an AI system can be described as performing some actions such as perception, observation, representation, prediction, or decision-making differently on certain objects, people, or groups compared to others. Bias can be categorized as desired/intended bias and unintended/unwanted. The document ISO/IEC TR 24027 helps you , understand from where the unintended bias is added to the AI systems: potential sources of unwanted bias are human cognitive bias, data bias, and bias introduced by engineering decisions. assessing bias and fairness: AI systems are complex and can be difficult to understand, but still, there exist strategies for the identification of unwanted bias. One way to uncover evidence of unwanted bias is to assess the system\u2019s outputs using one or more fairness metrics. The document presents several of such matricesmetrics. treat bias-related vulnerabilities throughout the entire AI system lifecycle , i.e., data collection, training, continual learning, design, testing, evaluation, and use. ISO/IEC TR 24028:2020, Information technology \u2013 Artificial intelligence \u2013 Overview of trustworthiness in artificial intelligence ISO/IEC TR 24028 surveys topics related to trustworthiness in AI systems, including: * Approaches to establish trust in AI systems. * Engineering pitfalls and typical associated threats and risks to AI systems, along with possible mitigation techniques and methods. * Approaches to assess and achieve availability, resiliency, reliability, accuracy, safety, security and privacy of AI systems. It does not specify levels of trustworthiness. ISO/IEC TR 24029-1:2021, Artificial Intelligence (AI) \u2014 Assessment of the robustness of neural networks Neural Networks (NNs) are being widely used due to the promising results on various complex pattern learning tasks. Statistical analyses are used to measure the performance of NNs under varying conditions. Additionally, some form of formal analysis using formal methods and empirical analysis using empirical methods are also required to access the robustness of the NNs. In this regard, ISO/IEC TR 24029 is aimed at helping AI engineers and users to assess the robustness of NNs throughout their life cycle. More specifically, part 1 of the document helps understand the risks tied to the robustness of AI systems while part 2 focuses more on providing recommendations and requirements for the use of formal methods to assess the robustness. Overall, there are six steps in the assessment process. As the first step, robustness goals are stated . Generally, the goals are not only for the statistical analysis but also for the formal and empirical analysis. Then in the second and third steps, testing is planned and conducted . Once completed, the outcomes are analyzed in the fourth step, and the results are interpreted in the fifth step. Finally, in the last step, the decision on the system robustness is formulated* by comparing the interpreted results obtained in the fifth step and the robustness goals stated in the first step. Functional safety ISO/IEC AWI TR 5469, Artificial intelligence \u2014 Functional safety and AI systems (This technical report is under development) The purpose of ISO/IEC TR 5469 is to enable the developer of safety-related systems to appropriately apply AI technologies as part of safety functions by fostering awareness of the properties, safety risk (in the context of functional safety) factors, available methods and potential constraints of AI technologies. It does so by: Giving an overview of functional safety and its relevance for AI. Describing the use of AI technology in safety-related programmable systems. Providing a classification scheme for the applicability of AI in safety-related programmable systems. Explaining AI technology elements and the three-stage realization principle. Explaining properties of AI systems and how they relate to safety in the context of functional safety. Discussing verification and validation techniques. Describing control and mitigation measures. Showing how IEC 61508 3 can be interpreted for applying it to AI, and providing alternative ways for compliance when possible. Mapping AI life cycle models to IEC 61508-1. Governance implications ISO/IEC 38507:2022, Information technology \u2014 Governance of IT \u2014 Governance implications of the use of artificial intelligence by organizations The objective of this document is to provide guidance for the governing body of an organization that is using, or is considering the use of, artificial intelligence (AI). \u201cUse of AI\u201d is defined in the broadest sense as developing or applying an AI system through any part of its life cycle to fulfil objectives and create value for the organization. A governing body can consider deploying AI in order to pursue specific opportunities that the organization has identified. In such cases, the governing body needs to weigh those opportunities against risk and other implications of use. New implications that arise from the use of AI could include: * increased reliance on technology * transparency and explainability issues * differences in assumptions made when delegating tasks to humans vs AI * competitive pressure of an organization not using AI * unawareness of potential bias, errors or harms of embedding AI into existing complex systems * disparity in speed of change between automated learning systems human controls of compliance; * the impact of AI on the workforce * the impact of AI on commercial operations and to brand reputation. The governing body should take responsibility for the use of AI, rather than attributing responsibility to the AI system itself. Members of the governing body are responsible for informing themselves about the possibilities and risks raised by using AI systems. Members of the governing body are accountable for the use of AI considered acceptable by the organization. The use of AI can result in new obligations for the organization. These can be legal requirements or as a consequence of the adoption of voluntary codes of practice, whether directly within an AI system\u2019s automation of decision-making processes or indirectly through its use of data or other resources or processes. In section 5.5, the document describes actions that the organization may take to constrain the use of AI: - Increase oversight of compliance. - Address the scope of use. - Assess and address the impact on stakeholders. - Determine legal requirements or obligations of using such technology. - Align the use of AI to the objectives of the organization. - Align the use of AI to the organization\u2019s culture and values. - Ensure that problem solving takes due account of context. - Examine the additional risk that the use of AI can bring to the organization. Other ISO/IEC TR 24372:2021, Information technology \u2014 Artificial intelligence (AI) \u2014 Overview of computational approaches for AI systems ISO/IEC TR 24372 describes the typical characteristics of AI systems (e.g. adaptable, explainable, discriminative) and their computational characteristics (e.g. data-based, knowledge-base, infrastructure-based). It describes two categories of computational approaches: knowledge-driven and data-driven. It describes selected algorithms and approaches related to: knowledge engineering and representation, logic and reasoning, machine learning, and metaheuristics. ISO/IEC TR 24030:2021, Information technology \u2014 Artificial intelligence (AI) \u2014 Use cases ISO/IEC TR 24030 provides a collection of use cases of artificial intelligence (AI) applications in a variety of domains, such as: agriculture, digital marketing, education, energy, financial markets, healthcare, robotics, ICT, legal, logistics, manufacturing, public sector, security, and transportation. It aims to illustrate the applicability of the AI standardization work across a variety of application domains, and share the collected use cases in support of AI standardization work with external organizations and internal entities to foster collaboration. It also intends to help identify new technical requirements that may accelerate the development of AI technology. ISO/IEC TR 29119-11:2020, Software and systems engineering \u2014 Software testing \u2014 Part 11: Guidelines on the testing of AI-based systems The testing of traditional systems is well-understood, but AI-based systems, which are becoming more prevalent and critical to our daily lives, introduce new challenges. This document has been created to introduce AI-based systems and provide guidelines on how they might be tested. This document explains those characteristics which are specific to AI-based systems and explains the corresponding difficulties of specifying the acceptance criteria for such systems. This document presents the challenges of testing AI-based systems, the main challenge being the test oracle problem, whereby testers find it difficult to determine expected results for testing and therefore whether tests have passed or failed. It covers testing of these systems across the life cycle and gives guidelines on how AI-based systems in general can be tested using black-box approaches and introduces white-box testing specifically for neural networks. It describes options for the test environments and test scenarios used for testing AI-based systems.","title":"ISO/IEC"},{"location":"iso/#isoiec-standardization-of-ai","text":"The ISO/IEC JTC 1/SC 42 comittee is responsible for standardization within the area of Artificial Intelligence. There are 11 standards published and 37 standards under development under the direct responsibility of this technical committee. The major ISO/IEC standards related to AI are presented below.","title":"ISO/IEC standardization of AI"},{"location":"iso/#foundational-standards","text":"","title":"Foundational standards"},{"location":"iso/#isoiec-22989-artificail-intelligence-concepts-and-terminology","text":"This document provides standardized concepts and terminology to help artificial intelligence technology be better understood and adopted by a broad set of stakeholders. This document can be used in the development of other standards and in support of communications among diverse, interested parties/stakeholders.","title":"ISO/IEC 22989, Artificail Intelligence - Concepts and Terminology"},{"location":"iso/#isoiec-23053-framework-for-artificial-intelligence-systems-using-machine-learning","text":"This document aims to provide a framework for the description of AI systems that use ML. By establishing a common terminology and a common set of concepts for such systems, this document provides a basis for clear explanation of the systems and various considerations that apply to their engineering and to their use. The document describes machine learning systems (chapter 6), machine learning approaches (chapter 7), The machine learning pipeline (chapter 8) and the machine learning process (chapter 9). Annex A maps ISO/IEC 23053 data types to ISO/IEC 19944-1 data categories.","title":"ISO/IEC 23053, Framework for Artificial Intelligence Systems Using Machine Learning"},{"location":"iso/#trustworthyness","text":"","title":"Trustworthyness"},{"location":"iso/#isoiec-prf-tr-24368-information-technology-artificial-intelligence-overview-of-ethical-and-societal-concerns","text":"This document provides a high-level overview of AI ethical and societal concerns. Examples of ethical and societal concerns in AI include privacy and security breaches to discriminatory outcomes and impact on human autonomy. The values embedded in algorithms, as well as the choice of problems AI systems and applications are used for to address, can be intentionally or inadvertently shaped by developers\u2019 and stakeholders\u2019 own worldviews and cognitive bias. Ethical and social concerns may arise even if an AI system performs flawlessly from a technical perspective. The document describes ethical frameworks, human rights practices, and principles for responsible use of AI. Appendix A is informative and gives an overview and summary of principles for ethical AI from around the world. Appendix B is informative and elaborates on ethical and societal concerns in several use cases.","title":"ISO/IEC PRF TR 24368, Information technology \u2014 Artificial intelligence \u2014 Overview of ethical and societal concerns"},{"location":"iso/#isoiec-cd-23894-information-technology-artificial-intelligence-risk-management","text":"AI systems can introduce new or emergent risks for an organization, with positive or negative consequences on objectives, or changes in the likelihood of existing risks. This document provides guidelines on how organizations that develop, produce, deploy or use products, systems and services that utilize AI can manage risk specifically related to AI. It uses ISO 31000:2018 as a normative reference and is intended to be used in connection with ISO 31000:2018. The clause structure of ISO 31000:2018 is mirrored in this document and amended by sub-clauses if needed. The main parts in the document are Clause 5 (principles), clause 5 (framework) and clause 6 (processes). Common AI-related objectives and risk sources are provided in Annex A and Annex B. Annex C provides an example mapping between the risk management processes and an AI system life cycle.","title":"ISO/IEC CD 23894 Information Technology \u2013 artificial Intelligence \u2013 risk management"},{"location":"iso/#isoiec-tr-240272021-information-technology-artificial-intelligence-ai-bias-in-ai-systems-and-ai-aided-decision-making","text":"Bias in an AI system can be described as performing some actions such as perception, observation, representation, prediction, or decision-making differently on certain objects, people, or groups compared to others. Bias can be categorized as desired/intended bias and unintended/unwanted. The document ISO/IEC TR 24027 helps you , understand from where the unintended bias is added to the AI systems: potential sources of unwanted bias are human cognitive bias, data bias, and bias introduced by engineering decisions. assessing bias and fairness: AI systems are complex and can be difficult to understand, but still, there exist strategies for the identification of unwanted bias. One way to uncover evidence of unwanted bias is to assess the system\u2019s outputs using one or more fairness metrics. The document presents several of such matricesmetrics. treat bias-related vulnerabilities throughout the entire AI system lifecycle , i.e., data collection, training, continual learning, design, testing, evaluation, and use.","title":"ISO/IEC TR 24027:2021, Information technology \u2014 Artificial intelligence (AI) \u2014 Bias in AI systems and AI aided decision making"},{"location":"iso/#isoiec-tr-240282020-information-technology-artificial-intelligence-overview-of-trustworthiness-in-artificial-intelligence","text":"ISO/IEC TR 24028 surveys topics related to trustworthiness in AI systems, including: * Approaches to establish trust in AI systems. * Engineering pitfalls and typical associated threats and risks to AI systems, along with possible mitigation techniques and methods. * Approaches to assess and achieve availability, resiliency, reliability, accuracy, safety, security and privacy of AI systems. It does not specify levels of trustworthiness.","title":"ISO/IEC TR 24028:2020, Information technology \u2013 Artificial intelligence \u2013 Overview of trustworthiness in artificial intelligence"},{"location":"iso/#isoiec-tr-24029-12021-artificial-intelligence-ai-assessment-of-the-robustness-of-neural-networks","text":"Neural Networks (NNs) are being widely used due to the promising results on various complex pattern learning tasks. Statistical analyses are used to measure the performance of NNs under varying conditions. Additionally, some form of formal analysis using formal methods and empirical analysis using empirical methods are also required to access the robustness of the NNs. In this regard, ISO/IEC TR 24029 is aimed at helping AI engineers and users to assess the robustness of NNs throughout their life cycle. More specifically, part 1 of the document helps understand the risks tied to the robustness of AI systems while part 2 focuses more on providing recommendations and requirements for the use of formal methods to assess the robustness. Overall, there are six steps in the assessment process. As the first step, robustness goals are stated . Generally, the goals are not only for the statistical analysis but also for the formal and empirical analysis. Then in the second and third steps, testing is planned and conducted . Once completed, the outcomes are analyzed in the fourth step, and the results are interpreted in the fifth step. Finally, in the last step, the decision on the system robustness is formulated* by comparing the interpreted results obtained in the fifth step and the robustness goals stated in the first step.","title":"ISO/IEC TR 24029-1:2021, Artificial Intelligence (AI) \u2014 Assessment of the robustness of neural networks"},{"location":"iso/#functional-safety","text":"","title":"Functional safety"},{"location":"iso/#isoiec-awi-tr-5469-artificial-intelligence-functional-safety-and-ai-systems","text":"(This technical report is under development) The purpose of ISO/IEC TR 5469 is to enable the developer of safety-related systems to appropriately apply AI technologies as part of safety functions by fostering awareness of the properties, safety risk (in the context of functional safety) factors, available methods and potential constraints of AI technologies. It does so by: Giving an overview of functional safety and its relevance for AI. Describing the use of AI technology in safety-related programmable systems. Providing a classification scheme for the applicability of AI in safety-related programmable systems. Explaining AI technology elements and the three-stage realization principle. Explaining properties of AI systems and how they relate to safety in the context of functional safety. Discussing verification and validation techniques. Describing control and mitigation measures. Showing how IEC 61508 3 can be interpreted for applying it to AI, and providing alternative ways for compliance when possible. Mapping AI life cycle models to IEC 61508-1.","title":"ISO/IEC AWI TR 5469, Artificial intelligence \u2014 Functional safety and AI systems"},{"location":"iso/#governance-implications","text":"","title":"Governance implications"},{"location":"iso/#isoiec-385072022-information-technology-governance-of-it-governance-implications-of-the-use-of-artificial-intelligence-by-organizations","text":"The objective of this document is to provide guidance for the governing body of an organization that is using, or is considering the use of, artificial intelligence (AI). \u201cUse of AI\u201d is defined in the broadest sense as developing or applying an AI system through any part of its life cycle to fulfil objectives and create value for the organization. A governing body can consider deploying AI in order to pursue specific opportunities that the organization has identified. In such cases, the governing body needs to weigh those opportunities against risk and other implications of use. New implications that arise from the use of AI could include: * increased reliance on technology * transparency and explainability issues * differences in assumptions made when delegating tasks to humans vs AI * competitive pressure of an organization not using AI * unawareness of potential bias, errors or harms of embedding AI into existing complex systems * disparity in speed of change between automated learning systems human controls of compliance; * the impact of AI on the workforce * the impact of AI on commercial operations and to brand reputation. The governing body should take responsibility for the use of AI, rather than attributing responsibility to the AI system itself. Members of the governing body are responsible for informing themselves about the possibilities and risks raised by using AI systems. Members of the governing body are accountable for the use of AI considered acceptable by the organization. The use of AI can result in new obligations for the organization. These can be legal requirements or as a consequence of the adoption of voluntary codes of practice, whether directly within an AI system\u2019s automation of decision-making processes or indirectly through its use of data or other resources or processes. In section 5.5, the document describes actions that the organization may take to constrain the use of AI: - Increase oversight of compliance. - Address the scope of use. - Assess and address the impact on stakeholders. - Determine legal requirements or obligations of using such technology. - Align the use of AI to the objectives of the organization. - Align the use of AI to the organization\u2019s culture and values. - Ensure that problem solving takes due account of context. - Examine the additional risk that the use of AI can bring to the organization.","title":"ISO/IEC 38507:2022, Information technology \u2014 Governance of IT \u2014 Governance implications of the use of artificial intelligence by organizations"},{"location":"iso/#other","text":"","title":"Other"},{"location":"iso/#isoiec-tr-243722021-information-technology-artificial-intelligence-ai-overview-of-computational-approaches-for-ai-systems","text":"ISO/IEC TR 24372 describes the typical characteristics of AI systems (e.g. adaptable, explainable, discriminative) and their computational characteristics (e.g. data-based, knowledge-base, infrastructure-based). It describes two categories of computational approaches: knowledge-driven and data-driven. It describes selected algorithms and approaches related to: knowledge engineering and representation, logic and reasoning, machine learning, and metaheuristics.","title":"ISO/IEC TR 24372:2021, Information technology \u2014 Artificial intelligence (AI) \u2014 Overview of computational approaches for AI systems"},{"location":"iso/#isoiec-tr-240302021-information-technology-artificial-intelligence-ai-use-cases","text":"ISO/IEC TR 24030 provides a collection of use cases of artificial intelligence (AI) applications in a variety of domains, such as: agriculture, digital marketing, education, energy, financial markets, healthcare, robotics, ICT, legal, logistics, manufacturing, public sector, security, and transportation. It aims to illustrate the applicability of the AI standardization work across a variety of application domains, and share the collected use cases in support of AI standardization work with external organizations and internal entities to foster collaboration. It also intends to help identify new technical requirements that may accelerate the development of AI technology.","title":"ISO/IEC TR 24030:2021, Information technology \u2014 Artificial intelligence (AI) \u2014 Use cases"},{"location":"iso/#isoiec-tr-29119-112020-software-and-systems-engineering-software-testing-part-11-guidelines-on-the-testing-of-ai-based-systems","text":"The testing of traditional systems is well-understood, but AI-based systems, which are becoming more prevalent and critical to our daily lives, introduce new challenges. This document has been created to introduce AI-based systems and provide guidelines on how they might be tested. This document explains those characteristics which are specific to AI-based systems and explains the corresponding difficulties of specifying the acceptance criteria for such systems. This document presents the challenges of testing AI-based systems, the main challenge being the test oracle problem, whereby testers find it difficult to determine expected results for testing and therefore whether tests have passed or failed. It covers testing of these systems across the life cycle and gives guidelines on how AI-based systems in general can be tested using black-box approaches and introduces white-box testing specifically for neural networks. It describes options for the test environments and test scenarios used for testing AI-based systems.","title":"ISO/IEC TR 29119-11:2020, Software and systems engineering \u2014 Software testing \u2014 Part 11: Guidelines on the testing of AI-based systems"},{"location":"nz/","text":"Regulation of AI in New Zealand The approach of the New Zealand Government is to progressively incorporate regulation of AI into existing legislation and regulations, rather than developing new AI-specific regulations and legislation. New Zealand has an ICT Operations Assurance Framework . They define assurance as \"an independent and objective assessment that provides credible information to support decision-making\", and list six principles of good assurance: * Assurance by design * Flexible * Informs key decisions * Risk and outcomes based * Independent and impartial * Accountability The New Zealand government also provide guidance and templates for ICT operations assurance , consisting of the following steps: 1. Managing your ICT (Information and Communications Technology) risks 2. Developing your assurance plan 3. Maximizing the value of independent assurance 4. Ensuring high quality assurance information 5. Overseeing assurance activities and recommendations 6. Capturing lessons learned","title":"New Zealand"},{"location":"nz/#regulation-of-ai-in-new-zealand","text":"The approach of the New Zealand Government is to progressively incorporate regulation of AI into existing legislation and regulations, rather than developing new AI-specific regulations and legislation. New Zealand has an ICT Operations Assurance Framework . They define assurance as \"an independent and objective assessment that provides credible information to support decision-making\", and list six principles of good assurance: * Assurance by design * Flexible * Informs key decisions * Risk and outcomes based * Independent and impartial * Accountability The New Zealand government also provide guidance and templates for ICT operations assurance , consisting of the following steps: 1. Managing your ICT (Information and Communications Technology) risks 2. Developing your assurance plan 3. Maximizing the value of independent assurance 4. Ensuring high quality assurance information 5. Overseeing assurance activities and recommendations 6. Capturing lessons learned","title":"Regulation of AI in New Zealand"},{"location":"oecd/","text":"TBD https://oecd.ai/en/","title":"OECD"},{"location":"uk/","text":"Regulation of AI in the United Kingdom In 2021, the UK Centre for Data Ethics and Innovaion (CDEI) published a roadmap , which is the first of its kind, setting out the steps required to build a world-leading AI assurance ecosystem in the UK. Their vision is that the UK will have a thriving and effective AI assurance ecosystem within the next 5 years. To provide meaningful and reliable assurance for AI, organisations need to overcome: * An information problem: reliably evaluate evidence to assess whether an AI system is trustworthy. * A communication problem: communicate the evidence at the right level, to inform assurance users\u2019 views on whether to trust an AI system. Assurance helps to overcome both of these problems. The UK roadmap describes three roles in relation to AI assurance, i.e., the 1st party role (responsible party), the 2nd party role (assurance user) and the 3rd party role (assurance provider). The 3rd party conducts assessment, testing and verification of the AI -system for the first party, and provides information about trustworthiness of the AI-system to the 2nd party. This enables the 2nd party to have justified trust in the 1at party. In addition, the roadmap describes four sets of key actors in the Ai assurance ecosystem: * Simplified AI supply chain: AI developers, executives deploying AI systems, Frontline users and affected individuals. * AI assurance service roviders: Independent assurance providers and internal assurance teams. * Independent research: Academic researchers and journalists/activists. * Supporting structures: Government, regulators, standards bodies and accreditation/professional bodies. Based on the main roles of these four important groups of actors in the AI assurance ecosystem, the CDEI have identified six priority areas for developing an effective, mature AI assurance ecosystem: Demand for AI assurance: The AI supply chain will need to demand, and receive, reliable evidence about the risks of these technologies, so they can make responsible adoption decisions. An AI assurance market: A competitive, dynamic market of service providers is needed to provide the tools and services to create this reliable evidence in an efficient and effective way. Standards: Different kinds of standards are needed to build common language and scalable assessment techniques Professionalization: The governance and incentives for assurance service providers need to be trusted. Regulation: Beyond auditing and inspecting AI as part of their enforcement activity, regulators will play an important role in supporting the development of the broader AI assurance ecosystem. Regulation can help to enable assurance, by setting assurable requirements that enable organizations to manage their regulatory obligations. Assurance also helps regulators to achieve their objectives by empowering users of AI to achieve compliance and manage risk. Independent research: In addition to specialized assurance providers, standards, regulatory, industry and professional bodies, other independent actors can offer important services to the assurance ecosystem.","title":"UK"},{"location":"uk/#regulation-of-ai-in-the-united-kingdom","text":"In 2021, the UK Centre for Data Ethics and Innovaion (CDEI) published a roadmap , which is the first of its kind, setting out the steps required to build a world-leading AI assurance ecosystem in the UK. Their vision is that the UK will have a thriving and effective AI assurance ecosystem within the next 5 years. To provide meaningful and reliable assurance for AI, organisations need to overcome: * An information problem: reliably evaluate evidence to assess whether an AI system is trustworthy. * A communication problem: communicate the evidence at the right level, to inform assurance users\u2019 views on whether to trust an AI system. Assurance helps to overcome both of these problems. The UK roadmap describes three roles in relation to AI assurance, i.e., the 1st party role (responsible party), the 2nd party role (assurance user) and the 3rd party role (assurance provider). The 3rd party conducts assessment, testing and verification of the AI -system for the first party, and provides information about trustworthiness of the AI-system to the 2nd party. This enables the 2nd party to have justified trust in the 1at party. In addition, the roadmap describes four sets of key actors in the Ai assurance ecosystem: * Simplified AI supply chain: AI developers, executives deploying AI systems, Frontline users and affected individuals. * AI assurance service roviders: Independent assurance providers and internal assurance teams. * Independent research: Academic researchers and journalists/activists. * Supporting structures: Government, regulators, standards bodies and accreditation/professional bodies. Based on the main roles of these four important groups of actors in the AI assurance ecosystem, the CDEI have identified six priority areas for developing an effective, mature AI assurance ecosystem: Demand for AI assurance: The AI supply chain will need to demand, and receive, reliable evidence about the risks of these technologies, so they can make responsible adoption decisions. An AI assurance market: A competitive, dynamic market of service providers is needed to provide the tools and services to create this reliable evidence in an efficient and effective way. Standards: Different kinds of standards are needed to build common language and scalable assessment techniques Professionalization: The governance and incentives for assurance service providers need to be trusted. Regulation: Beyond auditing and inspecting AI as part of their enforcement activity, regulators will play an important role in supporting the development of the broader AI assurance ecosystem. Regulation can help to enable assurance, by setting assurable requirements that enable organizations to manage their regulatory obligations. Assurance also helps regulators to achieve their objectives by empowering users of AI to achieve compliance and manage risk. Independent research: In addition to specialized assurance providers, standards, regulatory, industry and professional bodies, other independent actors can offer important services to the assurance ecosystem.","title":"Regulation of AI in the United Kingdom"},{"location":"us/","text":"AI risk management framework in the US https://www.nist.gov/itl/ai-risk-management-framework#:~:text=The%20NIST%20Artificial%20Intelligence%20Risk%20Management%20Framework%20%28AI,and%20evaluation%20of%20AI%20products%2C%20services%2C%20and%20systems.","title":"US"},{"location":"us/#ai-risk-management-framework-in-the-us","text":"https://www.nist.gov/itl/ai-risk-management-framework#:~:text=The%20NIST%20Artificial%20Intelligence%20Risk%20Management%20Framework%20%28AI,and%20evaluation%20of%20AI%20products%2C%20services%2C%20and%20systems.","title":"AI risk management framework in the US"}]}