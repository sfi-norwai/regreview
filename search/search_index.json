{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"AI regulation and governance This section contains resources about the regulation and governance of artificial intelligence (AI). It covers policies in various jusrisdictions, as well as standards, guidelines, recommended practices and frameworks for the promotion and assurance of trustworthy AI. The contents are organised in the following sections: EU: This section summarises the proposed EU AI act. China: This section describes the principles for AI governance and standardisation framework in China. UK: This section descibes the UK roadmap to an effective AI assurance ecosystem. New Zealand: This section describes thye principles approach of the New Zealand government to AI governance. ISO/IEC: This section describes important AI related ISO/IEC standards that are published or under development. IEEE: TBD OECD: TBD DNV: TBD Definitions: This section dicuss important definitions related to AI and assurance of AI.","title":"Introduction"},{"location":"#ai-regulation-and-governance","text":"This section contains resources about the regulation and governance of artificial intelligence (AI). It covers policies in various jusrisdictions, as well as standards, guidelines, recommended practices and frameworks for the promotion and assurance of trustworthy AI. The contents are organised in the following sections: EU: This section summarises the proposed EU AI act. China: This section describes the principles for AI governance and standardisation framework in China. UK: This section descibes the UK roadmap to an effective AI assurance ecosystem. New Zealand: This section describes thye principles approach of the New Zealand government to AI governance. ISO/IEC: This section describes important AI related ISO/IEC standards that are published or under development. IEEE: TBD OECD: TBD DNV: TBD Definitions: This section dicuss important definitions related to AI and assurance of AI.","title":"AI regulation and governance"},{"location":"china/","text":"AI governance and standardization in China In 2019, China's New Generation AI Governance Expert Committee published \"Eight principles for AI governance and \u201cresponsible AI\u201d . The motivation behind the proposed principles is the healthy development of a new generation of AI. If followed, one can ensure that AI is safe/secure, and reliable. The eight principles are as follows: Harmony and friendliness: The primary objective of AI should be to enhance the common well-being of humanity. In other words, AI should serve the progress of human civilization while complying with the ethical and moral dimensions, and upholding the societal security and human rights. Fairness and justice: Biasness and discrimination should be avoided in each phase from data gathering to the final product development of AI. Instead, fairness, justice, and equality of opportunity should be promoted. The rights and interests of stakeholders should be protected. Inclusivity and sharing: AI should be for everyone. Coordinated, shared, and inclusive development of AI should be encouraged to secure the adaptability of people from different backgrounds and abilities. Resources should be openly accessible to evade data and platform monopolies. Respect privacy: In the development of AI, personal privacy should be strictly protected. If personal data is used, necessary boundaries and standards should be enforced to protect them. Contributing individuals should have the right to know and decide which data should be used and where they should be used. Secure/safe and controllable: In order to attain trustworthiness, AI systems should emphasize transparency, explainability, reliability, and controllability. The robustness, safety, and security of AI systems must be given special attention, and an external assessment of these components is needed. Shared responsibility: In general, it is the responsibility of AI developers, users, beneficiaries, and all the stakeholders involved to prevent the misuse of AI against laws, regulations, ethics, morals, standards, and norms. When needed, the specific responsibilities of these parties should also be defined. Open collaboration: To promote the healthy development of AI, knowledge and ideas should be shared and exchanged across disciplines, domains, regions, and borders. For collaborations at the international level, consensus on an international AI governance framework, standards, and norms should be initiated. Agile governance: In order to promote the innovative, orderly, and natural development of AI, management mechanisms, governance systems, and recommender practices should be continuously updated. Future risks associated with the advancements in AI should be anticipated and necessary adjustments should be made accordingly. China has a multi-layered framework for standardisation of AI . Level A - Foundations: Foundational standards include the four main categories of terminology, reference architecture, data, and testing and assessment. These standards supports the other parts in the standards system structure. Level B - Platforms/support: Supportive technology and product standards provide basic support for artificial intelligence software and hardware platform construction, algorithm model development, and artificial intelligence applications. Fundamental hardware and software platform standards mainly focus on intelligent chips, system software, development frameworks, etc., to provide infrastructure support for artificial intelligence. Level C - Key technologies: Key technology standards a cover topics like natural language processing, human-computer interaction, computer vision, biometric feature recognition, and VR/AR. These standards provide support for practical applications of AI. Level D - Products and services: Products/services standards cover 'intelligentized' products and new service models formed in AI technology fields. Level E - Application standard: These standards focus on specific application areas, e.g. smart homes, smart manufacturing, smart healthcare, etc. They focus on needs facing the specific industries, refining other parts of the standards to support the development of various industries. Level F - Security/ethics: These standards run through the other parts to establish a compliance system for AI.","title":"China"},{"location":"china/#ai-governance-and-standardization-in-china","text":"In 2019, China's New Generation AI Governance Expert Committee published \"Eight principles for AI governance and \u201cresponsible AI\u201d . The motivation behind the proposed principles is the healthy development of a new generation of AI. If followed, one can ensure that AI is safe/secure, and reliable. The eight principles are as follows: Harmony and friendliness: The primary objective of AI should be to enhance the common well-being of humanity. In other words, AI should serve the progress of human civilization while complying with the ethical and moral dimensions, and upholding the societal security and human rights. Fairness and justice: Biasness and discrimination should be avoided in each phase from data gathering to the final product development of AI. Instead, fairness, justice, and equality of opportunity should be promoted. The rights and interests of stakeholders should be protected. Inclusivity and sharing: AI should be for everyone. Coordinated, shared, and inclusive development of AI should be encouraged to secure the adaptability of people from different backgrounds and abilities. Resources should be openly accessible to evade data and platform monopolies. Respect privacy: In the development of AI, personal privacy should be strictly protected. If personal data is used, necessary boundaries and standards should be enforced to protect them. Contributing individuals should have the right to know and decide which data should be used and where they should be used. Secure/safe and controllable: In order to attain trustworthiness, AI systems should emphasize transparency, explainability, reliability, and controllability. The robustness, safety, and security of AI systems must be given special attention, and an external assessment of these components is needed. Shared responsibility: In general, it is the responsibility of AI developers, users, beneficiaries, and all the stakeholders involved to prevent the misuse of AI against laws, regulations, ethics, morals, standards, and norms. When needed, the specific responsibilities of these parties should also be defined. Open collaboration: To promote the healthy development of AI, knowledge and ideas should be shared and exchanged across disciplines, domains, regions, and borders. For collaborations at the international level, consensus on an international AI governance framework, standards, and norms should be initiated. Agile governance: In order to promote the innovative, orderly, and natural development of AI, management mechanisms, governance systems, and recommender practices should be continuously updated. Future risks associated with the advancements in AI should be anticipated and necessary adjustments should be made accordingly. China has a multi-layered framework for standardisation of AI . Level A - Foundations: Foundational standards include the four main categories of terminology, reference architecture, data, and testing and assessment. These standards supports the other parts in the standards system structure. Level B - Platforms/support: Supportive technology and product standards provide basic support for artificial intelligence software and hardware platform construction, algorithm model development, and artificial intelligence applications. Fundamental hardware and software platform standards mainly focus on intelligent chips, system software, development frameworks, etc., to provide infrastructure support for artificial intelligence. Level C - Key technologies: Key technology standards a cover topics like natural language processing, human-computer interaction, computer vision, biometric feature recognition, and VR/AR. These standards provide support for practical applications of AI. Level D - Products and services: Products/services standards cover 'intelligentized' products and new service models formed in AI technology fields. Level E - Application standard: These standards focus on specific application areas, e.g. smart homes, smart manufacturing, smart healthcare, etc. They focus on needs facing the specific industries, refining other parts of the standards to support the development of various industries. Level F - Security/ethics: These standards run through the other parts to establish a compliance system for AI.","title":"AI governance and standardization in China"},{"location":"definitions/","text":"TBD","title":"Definitions"},{"location":"dnv/","text":"TBD","title":"DNV"},{"location":"eu/","text":"TBD","title":"EU"},{"location":"ieee/","text":"TBD","title":"IEEE"},{"location":"iso/","text":"ISO/IEC standardization of AI The ISO/IEC JTC 1/SC 42 comittee is responsible for standardization within the area of Artificial Intelligence. There are 11 standards published and 37 standards under development under the direct responsibility of this technical committee. The major ISO/IEC standards related to AI are presented below. Foundational standards ISO/IEC 22989, Artificail Intelligence - Concepts and Terminology TBD (Andreas,just one sentence?) ISO/IEC 23053, Framework for Artificial Intelligence Systems Using Machine Learning TBD (Andreas) Trustworthyness ISO/IEC PRF TR 24368, Information technology \u2014 Artificial intelligence \u2014 Overview of ethical and societal concerns TBD (Andreas) ISO/IEC CD 23894 Information Technology \u2013 artificial Intelligence \u2013 risk management TBD (Andreas) ISO/IEC TR 24027:2021, Information technology \u2014 Artificial intelligence (AI) \u2014 Bias in AI systems and AI aided decision making TBD (Andreas) ISO/IEC TR 24028:2020, Information technology \u2013 Artificial intelligence \u2013 Overview of trustworthiness in artificial intelligence TBD (Meine) ISO/IEC TR 24029-1:2021, Artificial Intelligence (AI) \u2014 Assessment of the robustness of neural networks Neural Networks (NNs) are being widely used due to the promising results on various complex pattern learning tasks. Statistical analyses are used to measure the performance of NNs under varying conditions. Additionally, some form of formal analysis using formal methods and empirical analysis using empirical methods are also required to access the robustness of the NNs. In this regard, ISO/IEC TR 24029 is aimed at helping AI engineers and users to assess the robustness of NNs throughout their life cycle. More specifically, part 1 of the document helps understand the risks tied to the robustness of AI systems while part 2 focuses more on providing recommendations and requirements for the use of formal methods to assess the robustness. Overall, there are six steps in the assessment process. As the first step, robustness goals are stated . Generally, the goals are not only for the statistical analysis but also for the formal and empirical analysis. Then in the second and third steps, testing is planned and conducted . Once completed, the outcomes are analyzed in the fourth step, and the results are interpreted in the fifth step. Finally, in the last step, the decision on the system robustness is formulated* by comparing the interpreted results obtained in the fifth step and the robustness goals stated in the first step. Functional safety ISO/IEC AWI TR 5469, Artificial intelligence \u2014 Functional safety and AI systems (This technical report is under development) The purpose of ISO/IEC TR 5469 is to enable the developer of safety-related systems to appropriately apply AI technologies as part of safety functions by fostering awareness of the properties, safety risk (in the context of functional safety) factors, available methods and potential constraints of AI technologies. It does so by: Giving an overview of functional safety and its relevance for AI. Describing the use of AI technology in safety-related programmable systems. Providing a classification scheme for the applicability of AI in safety-related programmable systems. Explaining AI technology elements and the three-stage realization principle. Explaining properties of AI systems and how they relate to safety in the context of functional safety. Discussing verification and validation techniques. Describing control and mitigation measures. Showing how IEC 61508 3 can be interpreted for applying it to AI, and providing alternative ways for compliance when possible. Mapping AI life cycle models to IEC 61508-1. Governance implications ISO/IEC 38507:2022, Information technology \u2014 Governance of IT \u2014 Governance implications of the use of artificial intelligence by organizations The objective of this document is to provide guidance for the governing body of an organization that is using, or is considering the use of, artificial intelligence (AI). \u201cUse of AI\u201d is defined in the broadest sense as developing or applying an AI system through any part of its life cycle to fulfil objectives and create value for the organization. A governing body can consider deploying AI in order to pursue specific opportunities that the organization has identified. In such cases, the governing body needs to weigh those opportunities against risk and other implications of use. New implications that arise from the use of AI could include: * increased reliance on technology * transparency and explainability issues * differences in assumptions made when delegating tasks to humans vs AI * competitive pressure of an organization not using AI * unawareness of potential bias, errors or harms of embedding AI into existing complex systems * disparity in speed of change between automated learning systems human controls of compliance; * the impact of AI on the workforce * the impact of AI on commercial operations and to brand reputation. The governing body should take responsibility for the use of AI, rather than attributing responsibility to the AI system itself. Members of the governing body are responsible for informing themselves about the possibilities and risks raised by using AI systems. Members of the governing body are accountable for the use of AI considered acceptable by the organization. The use of AI can result in new obligations for the organization. These can be legal requirements or as a consequence of the adoption of voluntary codes of practice, whether directly within an AI system\u2019s automation of decision-making processes or indirectly through its use of data or other resources or processes. In section 5.5, the document describes actions that the organization may take to constrain the use of AI: - Increase oversight of compliance. - Address the scope of use. - Assess and address the impact on stakeholders. - Determine legal requirements or obligations of using such technology. - Align the use of AI to the objectives of the organization. - Align the use of AI to the organization\u2019s culture and values. - Ensure that problem solving takes due account of context. - Examine the additional risk that the use of AI can bring to the organization. Other ISO/IEC TR 24372:2021, Information technology \u2014 Artificial intelligence (AI) \u2014 Overview of computational approaches for AI systems TBD (Meine) ISO/IEC TR 24030:2021, Information technology \u2014 Artificial intelligence (AI) \u2014 Use cases TBD (Meine)","title":"ISO/IEC"},{"location":"iso/#isoiec-standardization-of-ai","text":"The ISO/IEC JTC 1/SC 42 comittee is responsible for standardization within the area of Artificial Intelligence. There are 11 standards published and 37 standards under development under the direct responsibility of this technical committee. The major ISO/IEC standards related to AI are presented below.","title":"ISO/IEC standardization of AI"},{"location":"iso/#foundational-standards","text":"","title":"Foundational standards"},{"location":"iso/#isoiec-22989-artificail-intelligence-concepts-and-terminology","text":"TBD (Andreas,just one sentence?)","title":"ISO/IEC 22989, Artificail Intelligence - Concepts and Terminology"},{"location":"iso/#isoiec-23053-framework-for-artificial-intelligence-systems-using-machine-learning","text":"TBD (Andreas)","title":"ISO/IEC 23053, Framework for Artificial Intelligence Systems Using Machine Learning"},{"location":"iso/#trustworthyness","text":"","title":"Trustworthyness"},{"location":"iso/#isoiec-prf-tr-24368-information-technology-artificial-intelligence-overview-of-ethical-and-societal-concerns","text":"TBD (Andreas)","title":"ISO/IEC PRF TR 24368, Information technology \u2014 Artificial intelligence \u2014 Overview of ethical and societal concerns"},{"location":"iso/#isoiec-cd-23894-information-technology-artificial-intelligence-risk-management","text":"TBD (Andreas)","title":"ISO/IEC CD 23894 Information Technology \u2013 artificial Intelligence \u2013 risk management"},{"location":"iso/#isoiec-tr-240272021-information-technology-artificial-intelligence-ai-bias-in-ai-systems-and-ai-aided-decision-making","text":"TBD (Andreas)","title":"ISO/IEC TR 24027:2021, Information technology \u2014 Artificial intelligence (AI) \u2014 Bias in AI systems and AI aided decision making"},{"location":"iso/#isoiec-tr-240282020-information-technology-artificial-intelligence-overview-of-trustworthiness-in-artificial-intelligence","text":"TBD (Meine)","title":"ISO/IEC TR 24028:2020, Information technology \u2013 Artificial intelligence \u2013 Overview of trustworthiness in artificial intelligence"},{"location":"iso/#isoiec-tr-24029-12021-artificial-intelligence-ai-assessment-of-the-robustness-of-neural-networks","text":"Neural Networks (NNs) are being widely used due to the promising results on various complex pattern learning tasks. Statistical analyses are used to measure the performance of NNs under varying conditions. Additionally, some form of formal analysis using formal methods and empirical analysis using empirical methods are also required to access the robustness of the NNs. In this regard, ISO/IEC TR 24029 is aimed at helping AI engineers and users to assess the robustness of NNs throughout their life cycle. More specifically, part 1 of the document helps understand the risks tied to the robustness of AI systems while part 2 focuses more on providing recommendations and requirements for the use of formal methods to assess the robustness. Overall, there are six steps in the assessment process. As the first step, robustness goals are stated . Generally, the goals are not only for the statistical analysis but also for the formal and empirical analysis. Then in the second and third steps, testing is planned and conducted . Once completed, the outcomes are analyzed in the fourth step, and the results are interpreted in the fifth step. Finally, in the last step, the decision on the system robustness is formulated* by comparing the interpreted results obtained in the fifth step and the robustness goals stated in the first step.","title":"ISO/IEC TR 24029-1:2021, Artificial Intelligence (AI) \u2014 Assessment of the robustness of neural networks"},{"location":"iso/#functional-safety","text":"","title":"Functional safety"},{"location":"iso/#isoiec-awi-tr-5469-artificial-intelligence-functional-safety-and-ai-systems","text":"(This technical report is under development) The purpose of ISO/IEC TR 5469 is to enable the developer of safety-related systems to appropriately apply AI technologies as part of safety functions by fostering awareness of the properties, safety risk (in the context of functional safety) factors, available methods and potential constraints of AI technologies. It does so by: Giving an overview of functional safety and its relevance for AI. Describing the use of AI technology in safety-related programmable systems. Providing a classification scheme for the applicability of AI in safety-related programmable systems. Explaining AI technology elements and the three-stage realization principle. Explaining properties of AI systems and how they relate to safety in the context of functional safety. Discussing verification and validation techniques. Describing control and mitigation measures. Showing how IEC 61508 3 can be interpreted for applying it to AI, and providing alternative ways for compliance when possible. Mapping AI life cycle models to IEC 61508-1.","title":"ISO/IEC AWI TR 5469, Artificial intelligence \u2014 Functional safety and AI systems"},{"location":"iso/#governance-implications","text":"","title":"Governance implications"},{"location":"iso/#isoiec-385072022-information-technology-governance-of-it-governance-implications-of-the-use-of-artificial-intelligence-by-organizations","text":"The objective of this document is to provide guidance for the governing body of an organization that is using, or is considering the use of, artificial intelligence (AI). \u201cUse of AI\u201d is defined in the broadest sense as developing or applying an AI system through any part of its life cycle to fulfil objectives and create value for the organization. A governing body can consider deploying AI in order to pursue specific opportunities that the organization has identified. In such cases, the governing body needs to weigh those opportunities against risk and other implications of use. New implications that arise from the use of AI could include: * increased reliance on technology * transparency and explainability issues * differences in assumptions made when delegating tasks to humans vs AI * competitive pressure of an organization not using AI * unawareness of potential bias, errors or harms of embedding AI into existing complex systems * disparity in speed of change between automated learning systems human controls of compliance; * the impact of AI on the workforce * the impact of AI on commercial operations and to brand reputation. The governing body should take responsibility for the use of AI, rather than attributing responsibility to the AI system itself. Members of the governing body are responsible for informing themselves about the possibilities and risks raised by using AI systems. Members of the governing body are accountable for the use of AI considered acceptable by the organization. The use of AI can result in new obligations for the organization. These can be legal requirements or as a consequence of the adoption of voluntary codes of practice, whether directly within an AI system\u2019s automation of decision-making processes or indirectly through its use of data or other resources or processes. In section 5.5, the document describes actions that the organization may take to constrain the use of AI: - Increase oversight of compliance. - Address the scope of use. - Assess and address the impact on stakeholders. - Determine legal requirements or obligations of using such technology. - Align the use of AI to the objectives of the organization. - Align the use of AI to the organization\u2019s culture and values. - Ensure that problem solving takes due account of context. - Examine the additional risk that the use of AI can bring to the organization.","title":"ISO/IEC 38507:2022, Information technology \u2014 Governance of IT \u2014 Governance implications of the use of artificial intelligence by organizations"},{"location":"iso/#other","text":"","title":"Other"},{"location":"iso/#isoiec-tr-243722021-information-technology-artificial-intelligence-ai-overview-of-computational-approaches-for-ai-systems","text":"TBD (Meine)","title":"ISO/IEC TR 24372:2021, Information technology \u2014 Artificial intelligence (AI) \u2014 Overview of computational approaches for AI systems"},{"location":"iso/#isoiec-tr-240302021-information-technology-artificial-intelligence-ai-use-cases","text":"TBD (Meine)","title":"ISO/IEC TR 24030:2021, Information technology \u2014 Artificial intelligence (AI) \u2014 Use cases"},{"location":"nz/","text":"Regulation of AI in New Zealand The approach of the New Zealand Government is to progressively incorporate regulation of AI into existing legislation and regulations, rather than developing new AI-specific regulations and legislation. New Zealand has an ICT Operations Assurance Framework . They define assurance as \"an independent and objective assessment that provides credible information to support decision-making\", and list six principles of good assurance: * Assurance by design * Flexible * Informs key decisions * Risk and outcomes based * Independent and impartial * Accountability The New Zealand government also provide guidance and templates for ICT operations assurance , consisting of the following steps: 1. Managing your ICT (Information and Communications Technology) risks 2. Developing your assurance plan 3. Maximizing the value of independent assurance 4. Ensuring high quality assurance information 5. Overseeing assurance activities and recommendations 6. Capturing lessons learned","title":"New Zealand"},{"location":"nz/#regulation-of-ai-in-new-zealand","text":"The approach of the New Zealand Government is to progressively incorporate regulation of AI into existing legislation and regulations, rather than developing new AI-specific regulations and legislation. New Zealand has an ICT Operations Assurance Framework . They define assurance as \"an independent and objective assessment that provides credible information to support decision-making\", and list six principles of good assurance: * Assurance by design * Flexible * Informs key decisions * Risk and outcomes based * Independent and impartial * Accountability The New Zealand government also provide guidance and templates for ICT operations assurance , consisting of the following steps: 1. Managing your ICT (Information and Communications Technology) risks 2. Developing your assurance plan 3. Maximizing the value of independent assurance 4. Ensuring high quality assurance information 5. Overseeing assurance activities and recommendations 6. Capturing lessons learned","title":"Regulation of AI in New Zealand"},{"location":"oecd/","text":"TBD","title":"OECD"},{"location":"uk/","text":"Regulation of AI in the United Kingdom In 2021, the UK Centre for Data Ethics and Innovaion (CDEI) published a roadmap , which is the first of its kind, setting out the steps required to build a world-leading AI assurance ecosystem in the UK. Their vision is that the UK will have a thriving and effective AI assurance ecosystem within the next 5 years. To provide meaningful and reliable assurance for AI, organisations need to overcome: * An information problem: reliably evaluate evidence to assess whether an AI system is trustworthy. * A communication problem: communicate the evidence at the right level, to inform assurance users\u2019 views on whether to trust an AI system. Assurance helps to overcome both of these problems. The UK roadmap describes three roles in relation to AI assurance, i.e., the 1st party role (responsible party), the 2nd party role (assurance user) and the 3rd party role (assurance provider). The 3rd party conducts assessment, testing and verification of the AI -system for the first party, and provides information about trustworthiness of the AI-system to the 2nd party. This enables the 2nd party to have justified trust in the 1at party. In addition, the roadmap describes four sets of key actors in the Ai assurance ecosystem: * Simplified AI supply chain: AI developers, executives deploying AI systems, Frontline users and affected individuals. * AI assurance service roviders: Independent assurance providers and internal assurance teams. * Independent research: Academic researchers and journalists/activists. * Supporting structures: Government, regulators, standards bodies and accreditation/professional bodies. Based on the main roles of these four important groups of actors in the AI assurance ecosystem, the CDEI have identified six priority areas for developing an effective, mature AI assurance ecosystem: Demand for AI assurance: The AI supply chain will need to demand, and receive, reliable evidence about the risks of these technologies, so they can make responsible adoption decisions. An AI assurance market: A competitive, dynamic market of service providers is needed to provide the tools and services to create this reliable evidence in an efficient and effective way. Standards: Different kinds of standards are needed to build common language and scalable assessment techniques Professionalization: The governance and incentives for assurance service providers need to be trusted. Regulation: Beyond auditing and inspecting AI as part of their enforcement activity, regulators will play an important role in supporting the development of the broader AI assurance ecosystem. Regulation can help to enable assurance, by setting assurable requirements that enable organizations to manage their regulatory obligations. Assurance also helps regulators to achieve their objectives by empowering users of AI to achieve compliance and manage risk. Independent research: In addition to specialized assurance providers, standards, regulatory, industry and professional bodies, other independent actors can offer important services to the assurance ecosystem.","title":"UK"},{"location":"uk/#regulation-of-ai-in-the-united-kingdom","text":"In 2021, the UK Centre for Data Ethics and Innovaion (CDEI) published a roadmap , which is the first of its kind, setting out the steps required to build a world-leading AI assurance ecosystem in the UK. Their vision is that the UK will have a thriving and effective AI assurance ecosystem within the next 5 years. To provide meaningful and reliable assurance for AI, organisations need to overcome: * An information problem: reliably evaluate evidence to assess whether an AI system is trustworthy. * A communication problem: communicate the evidence at the right level, to inform assurance users\u2019 views on whether to trust an AI system. Assurance helps to overcome both of these problems. The UK roadmap describes three roles in relation to AI assurance, i.e., the 1st party role (responsible party), the 2nd party role (assurance user) and the 3rd party role (assurance provider). The 3rd party conducts assessment, testing and verification of the AI -system for the first party, and provides information about trustworthiness of the AI-system to the 2nd party. This enables the 2nd party to have justified trust in the 1at party. In addition, the roadmap describes four sets of key actors in the Ai assurance ecosystem: * Simplified AI supply chain: AI developers, executives deploying AI systems, Frontline users and affected individuals. * AI assurance service roviders: Independent assurance providers and internal assurance teams. * Independent research: Academic researchers and journalists/activists. * Supporting structures: Government, regulators, standards bodies and accreditation/professional bodies. Based on the main roles of these four important groups of actors in the AI assurance ecosystem, the CDEI have identified six priority areas for developing an effective, mature AI assurance ecosystem: Demand for AI assurance: The AI supply chain will need to demand, and receive, reliable evidence about the risks of these technologies, so they can make responsible adoption decisions. An AI assurance market: A competitive, dynamic market of service providers is needed to provide the tools and services to create this reliable evidence in an efficient and effective way. Standards: Different kinds of standards are needed to build common language and scalable assessment techniques Professionalization: The governance and incentives for assurance service providers need to be trusted. Regulation: Beyond auditing and inspecting AI as part of their enforcement activity, regulators will play an important role in supporting the development of the broader AI assurance ecosystem. Regulation can help to enable assurance, by setting assurable requirements that enable organizations to manage their regulatory obligations. Assurance also helps regulators to achieve their objectives by empowering users of AI to achieve compliance and manage risk. Independent research: In addition to specialized assurance providers, standards, regulatory, industry and professional bodies, other independent actors can offer important services to the assurance ecosystem.","title":"Regulation of AI in the United Kingdom"}]}